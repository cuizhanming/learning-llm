{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understand GPT-2 model\n",
    "\n",
    "- BERT, 分类模型；\n",
    "- GPT-2, 生成模型；[Hugging Face Transformers/GPT2 Documents](https://huggingface.co/docs/transformers/en/model_doc/gpt2)"
   ],
   "id": "2b4d73aaf9f0578d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:20:42.950897Z",
     "start_time": "2025-01-04T11:20:41.952814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load model to local\n",
    "\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "# model_name = \"uer/gpt2-chinese-lyric\"\n",
    "model_name = \"uer/gpt2-chinese-poem\"\n",
    "cache_dir = \"../local_models\"\n",
    "AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ],
   "id": "980319ba6ea1aa2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/gpt2-chinese-poem', vocab_size=22557, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:21:53.220865Z",
     "start_time": "2025-01-04T11:21:51.755840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try the GPT2 model\n",
    "\n",
    "from transformers import GPT2LMHeadModel, BertTokenizer, TextGenerationPipeline\n",
    "\n",
    "# model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "# model_path=\"../local_models/models--uer--gpt2-chinese-lyric/snapshots/4a42fd76daab07d9d7ff95c816160cfb7c21684f\"\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-poem/snapshots/6335c88ef6a3362dcdf2e988577b7bafeda6052b\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer,device=\"cpu\")\n",
    "\n",
    "prompt = \"中文GPT2大规模预训练模型\"\n",
    "output = text_generator(prompt, max_length=100, do_sample=True)\n",
    "\n",
    "print(model)\n",
    "print(output)"
   ],
   "id": "ef8dfe4220bce624",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(22557, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=22557, bias=False)\n",
      ")\n",
      "[{'generated_text': '中文GPT2大规模预训练模型 ， 大 法 非 难 解 。 两 都 不 作 难 ， 万 事 长 交 缦 。 吾 意 欲 语 谁 ， 君 言 欲 下 拜 。 将 军 下 弓 入 ， 军 士 操 锋 散 。 中 原 久 丧 乱 ， 四 海 竞 奔 窜 。 君 能 贾 馀 勇 ， 为 国 先 除 乱 。 有 谋 足 胜 人 ， 无 事 且 游 观 。 此 弥 留 都 ，'}]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train a GPT-2 base model to be a Poem model:\n",
    "\n",
    "### Step 1: Load the dataset"
   ],
   "id": "f9b15418e740f746"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:24:40.237116Z",
     "start_time": "2025-01-04T11:24:40.119874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Here, we are just reading the file. You can add custom pre-processing here\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            text = f.readlines()\n",
    "        text = [i.strip() for i in text]\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.text[item]\n",
    "\n",
    "dataset = PoemDataset(\"../local_datasets/Poem/chinese_poems.txt\")\n",
    "\n",
    "for data in dataset[:5]:\n",
    "    print(data)"
   ],
   "id": "ad19b0c77af61a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.\n",
      "满目江山四望幽,白云高卷嶂烟收.日回禽影穿疏木,风递猿声入小楼.远岫似屏横碧落,断帆如叶截中流.\n",
      "片片飞来静又闲,楼头江上复山前.飘零尽日不归去,帖破清光万里天.\n",
      "因登巨石知来处,勃勃元生绿藓痕.静即等闲藏草木,动时顷刻徧乾坤.横天未必朋元恶,捧日还曾瑞至尊.不独朝朝在巫峡,楚王何事谩劳魂.\n",
      "一气东南王斗牛,祖龙潜为子孙忧.金陵地脉何曾断,不觉真人已姓刘.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Training the model\n",
    "\n",
    "- BERT, Incremental training model;\n",
    "- GPT-2, Full training model;"
   ],
   "id": "cc26cb6daa67ae02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:32:03.747940Z",
     "start_time": "2025-01-04T11:26:59.813921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "import torch\n",
    "\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset_train = PoemDataset(file_path=\"../local_datasets/Poem/chinese_poems.txt\")\n",
    "def collate_fn(data):\n",
    "    data = tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    data[\"labels\"] = data[\"input_ids\"].clone()\n",
    "    return data\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Dataset length: {len(dataset_train)}\")\n",
    "\n",
    "def train():\n",
    "    global model\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # lr 2e-5 - 5e-5\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(dataloader)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            for k in data.keys():\n",
    "                data[k] = data[k].to(DEVICE)\n",
    "            outputs = model(**data)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                labels = data[\"labels\"][:, 1:].contiguous() # Target\n",
    "                out = outputs[\"logits\"].argmax(dim=2)[:, :-1].contiguous() # Predictions\n",
    "                select = labels != 0 # Select all tokens that are not <PAD>\n",
    "                labels = labels[select]\n",
    "                out = out[select]\n",
    "                del select\n",
    "                accuracy = (labels == out).sum().item() / labels.numel()\n",
    "                lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "\n",
    "                print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}, lr: {lr}, Accuracy: {accuracy}\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"params/model.pt\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "train()"
   ],
   "id": "51a38c7ca6168719",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 304752\n",
      "Epoch: 0, Iteration: 0, Loss: 9.866423606872559, lr: 1.9999737491468474e-05, Accuracy: 0.06976744186046512\n",
      "Epoch: 0, Iteration: 100, Loss: 3.88360595703125, lr: 1.9973486638315748e-05, Accuracy: 0.18627450980392157\n",
      "Epoch: 0, Iteration: 200, Loss: 3.2421789169311523, lr: 1.9947235785163018e-05, Accuracy: 0.19230769230769232\n",
      "Epoch: 0, Iteration: 300, Loss: 4.859396457672119, lr: 1.9920984932010292e-05, Accuracy: 0.18292682926829268\n",
      "Epoch: 0, Iteration: 400, Loss: 4.392740726470947, lr: 1.9894734078857565e-05, Accuracy: 0.2012987012987013\n",
      "Epoch: 0, Iteration: 500, Loss: 5.057992935180664, lr: 1.986848322570484e-05, Accuracy: 0.19672131147540983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 75\u001B[0m\n\u001B[1;32m     72\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams/model.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     73\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel saved!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 51\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     49\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[1;32m     50\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[0;32m---> 51\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1.0\u001B[39m)\n\u001B[1;32m     53\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/Workspace/pyenv/envs/rag-embedding/lib/python3.10/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/pyenv/envs/rag-embedding/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/pyenv/envs/rag-embedding/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
