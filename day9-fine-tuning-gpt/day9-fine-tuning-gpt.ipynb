{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understand GPT-2 model\n",
    "\n",
    "- BERT, 分类模型；\n",
    "- GPT-2, 生成模型；[Hugging Face Transformers/GPT2 Documents](https://huggingface.co/docs/transformers/en/model_doc/gpt2)"
   ],
   "id": "2b4d73aaf9f0578d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.distributed.pipelining import pipeline\n",
    "# Load model to local\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "# model_name = \"uer/gpt2-chinese-lyric\"\n",
    "model_name = \"uer/gpt2-chinese-poem\"\n",
    "cache_dir = \"../local_models\"\n",
    "AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ],
   "id": "980319ba6ea1aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Try the GPT2 model\n",
    "from transformers import GPT2LMHeadModel, BertTokenizer, TextGenerationPipeline\n",
    "\n",
    "# model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "# model_path=\"../local_models/models--uer--gpt2-chinese-lyric/snapshots/4a42fd76daab07d9d7ff95c816160cfb7c21684f\"\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-poem/snapshots/6335c88ef6a3362dcdf2e988577b7bafeda6052b\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer,device=\"cpu\")\n",
    "\n",
    "prompt = \"中文GPT2大规模预训练模型\"\n",
    "output = text_generator(prompt, max_length=100, do_sample=True)\n",
    "\n",
    "print(model)\n",
    "print(output)"
   ],
   "id": "ef8dfe4220bce624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train a GPT-2 base model to be a Poem model:\n",
    "\n",
    "### Step 1: Load the dataset"
   ],
   "id": "f9b15418e740f746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Here, we are just reading the file. You can add custom pre-processing here\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            text = f.readlines()\n",
    "        text = [i.strip() for i in text]\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.text[item]\n",
    "\n",
    "dataset_train = PoemDataset(file_path=\"../local_datasets/Poem/chinese_poems.txt\")\n",
    "for data in dataset_train[:5]:\n",
    "    print(data)\n",
    "\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "def collate_fn(data):\n",
    "    data = tokenizer.batch_encode_plus(\n",
    "        data,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    data[\"labels\"] = data[\"input_ids\"].clone()\n",
    "    return data\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "print(f\"Dataset length: {len(dataset_train)}\")"
   ],
   "id": "ad19b0c77af61a66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Training the model\n",
    "\n",
    "- BERT, Incremental training model;\n",
    "- GPT-2, Full training model;"
   ],
   "id": "cc26cb6daa67ae02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "import torch\n",
    "\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "def run_train():\n",
    "    global model\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    EPOCH = 30000\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5) # lr 2e-5 - 5e-5\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(dataloader)\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(EPOCH):\n",
    "        for i, data in enumerate(dataloader):\n",
    "            for k in data.keys():\n",
    "                data[k] = data[k].to(DEVICE)\n",
    "            outputs = model(**data)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                labels = data[\"labels\"][:, 1:].contiguous() # Target\n",
    "                out = outputs[\"logits\"].argmax(dim=2)[:, :-1].contiguous() # Predictions\n",
    "                select = labels != 0 # Select all tokens that are not <PAD>\n",
    "                labels = labels[select]\n",
    "                out = out[select]\n",
    "                del select\n",
    "                accuracy = (labels == out).sum().item() / labels.numel()\n",
    "                lr = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
    "                if lr == 0.0:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group[\"lr\"] = 2e-5\n",
    "\n",
    "                print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}, lr: {lr}, Accuracy: {accuracy}\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"params/model.pt\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "# Trigger the training\n",
    "# run_train()"
   ],
   "id": "51a38c7ca6168719",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Generate the Poem with the fine-tuned model\n",
    "\n",
    "- After training the model, we saved the weight.\n",
    "- Now, we can load the weight and generate the poem."
   ],
   "id": "864022c0b527fe5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "model_path=\"../local_models/models--uer--gpt2-chinese-cluecorpussmall/snapshots/c2c0249d8a2731f269414cc3b22dff021f8e07a3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "trained_parameter_path = \"../local_params/net-2.pt\"\n",
    "loaded_object = torch.load(trained_parameter_path, map_location=torch.device(\"cpu\"))\n",
    "model.load_state_dict(loaded_object)\n",
    "\n",
    "# Default pipeline to generate\n",
    "pipeline = TextGenerationPipeline(model, tokenizer, device=0)\n",
    "print(pipeline(\"白\", max_length=24))\n",
    "\n",
    "# Custom pipeline to generate\n",
    "# 用于生成5言绝句 text是提示词，row是生成文本的行数，col是每行的字符数。\n",
    "def generate(text, row, col):\n",
    "\n",
    "    #定义一个内部递归函数，用于生成文本\n",
    "    def generate_loop(data):\n",
    "        #禁用梯度计算\n",
    "        with torch.no_grad():\n",
    "            #使用data字典中的数据作为模型输入，并获取输出\n",
    "            out = model(**data)\n",
    "        #获取最后一个字(logits未归一化的概率输出)\n",
    "        out = out[\"logits\"]\n",
    "        #选择每个序列的最后一个logits，对应于下一个词的预测\n",
    "        out = out[:,-1]\n",
    "\n",
    "        #找到概率排名前50的值，以此为分界线，小于该值的全部舍去\n",
    "        topk_value = torch.topk(out,50).values\n",
    "        #获取每个输出序列中前50个最大的logits（为保持原维度不变，需要对结果增加一个维度，因为索引操作会降维）\n",
    "        topk_value = topk_value[:,-1].unsqueeze(dim=1)\n",
    "        #将所有小于第50大的值的logits设置为负无穷，减少低概率词的选择\n",
    "        out = out.masked_fill(out < topk_value,-float(\"inf\"))\n",
    "\n",
    "        #将特殊符号的logits值设置为负无穷，防止模型生成这些符号。\n",
    "        for i in \",.()《《[]「」{}\":\n",
    "            out[:,tokenizer.get_vocab()[i]] = -float('inf')\n",
    "        out[:,tokenizer.get_vocab()[\"[PAD]\"]] = -float('inf')\n",
    "        out[:,tokenizer.get_vocab()[\"[UNK]\"]] = -float('inf')\n",
    "        out[:,tokenizer.get_vocab()[\"[CLS]\"]] = -float('inf')\n",
    "        out[:,tokenizer.get_vocab()[\"[SEP]\"]] = -float('inf')\n",
    "\n",
    "        #根据概率采样，无放回，避免生成重复的内容\n",
    "        out = out.softmax(dim=1)\n",
    "        #从概率分布中进行采样，选择下一个词的ID\n",
    "        out = out.multinomial(num_samples=1)\n",
    "\n",
    "        #强值添加标点符号\n",
    "        #计算当前生成的文本长度于预期的长度的比例\n",
    "        c = data[\"input_ids\"].shape[1] / (col+1)\n",
    "        #如果当前的长度是预期长度的整数倍，则添加标点符号\n",
    "        if c % 1 ==0:\n",
    "            if c % 2 ==0:\n",
    "                #在偶数位添加句号\n",
    "                out[:,0] = tokenizer.get_vocab()[\".\"]\n",
    "            else:\n",
    "                #在奇数位添加逗号\n",
    "                out[:,0] = tokenizer.get_vocab()[\",\"]\n",
    "        #将生成的新词ID添加到输入序列的末尾\n",
    "        data[\"input_ids\"] = torch.cat([data[\"input_ids\"],out],dim=1)\n",
    "        #更新注意力掩码，标记所有有效位置\n",
    "        data[\"attention_mask\"] = torch.ones_like(data[\"input_ids\"])\n",
    "        #更新token的ID类型，通常在BERTm模型中使用，但是在GPT模型中是不用的\n",
    "        data[\"token_type_ids\"] = torch.ones_like(data[\"input_ids\"])\n",
    "        #更新标签，这里将输入ID复制到标签中，在语言生成模型中通常用与预测下一个词\n",
    "        data[\"labels\"] = data[\"input_ids\"].clone()\n",
    "\n",
    "        #检查生成的文本长度是否达到或超过指定的行数和列数\n",
    "        if data[\"input_ids\"].shape[1] >= row*col + row+1:\n",
    "            #如果达到长度要求，则返回最终的data字典\n",
    "            return data\n",
    "        #如果长度未达到要求，递归调用generate_loop函数继续生成文本\n",
    "        return generate_loop(data)\n",
    "\n",
    "    #生成3首诗词\n",
    "    #使用tokenizer对输入文本进行编码，并重复3次生成3个样本。\n",
    "    data = tokenizer.batch_encode_plus([text] * 3, return_tensors=\"pt\")\n",
    "    #移除编码后的序列中的最后一个token(结束符号)\n",
    "    data[\"input_ids\"] = data[\"input_ids\"][:,:-1]\n",
    "    #创建一个与input_ids形状相同的全1张量，用于注意力掩码\n",
    "    data[\"attention_mask\"] = torch.ones_like(data[\"input_ids\"])\n",
    "    # 创建一个与input_ids形状相同的全0张量，用于token类型ID\n",
    "    data[\"token_type_ids\"] = torch.zeros_like(data[\"input_ids\"])\n",
    "    #复制input_ids到labels，用于模型的目标\n",
    "    data['labels'] = data[\"input_ids\"].clone()\n",
    "\n",
    "    #调用generate_loop函数开始生成文本\n",
    "    data = generate_loop(data)\n",
    "\n",
    "    #遍历生成的3个样本\n",
    "    for i in range(3):\n",
    "        #打印输出样本索引和对应的解码后的文本\n",
    "        print(i,tokenizer.decode(data[\"input_ids\"][i]))\n",
    "\n",
    "generate(\"白\",row=4,col=5)"
   ],
   "id": "6dcd123f30c95e14",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
