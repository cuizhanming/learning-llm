{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hugging Face BERT Fine-Tuning with PyTorch\n",
    "\n",
    "## 1. Fine-tuning Basics\n",
    "- 微调是指在预训练模型的基础上，通过进一步的训练来适应特定的下游任务。\n",
    "- BERT 模型通过预训练来学习语言的通用模式，然后通过微调来适应特定任务，如情感分析、命名实体识别等。\n",
    "- 微调过程中，通常冻结BERT的预训练层，只训练与下游任务相关的层。本课件将介绍如何使用BERT模型进行情感分析任务的微调训练。"
   ],
   "id": "e80c5bdd659f396a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. DataLoader\n",
    "- 情感分析任务的数据通常包括文本及其对应的情感标签。\n",
    "- 使用Hugging Face的datasets库可以轻松地加载和处理数据集。"
   ],
   "id": "2a9ec39572186a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from re import split\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from qianfan.common.cli.dataset import predict\n",
    "\n",
    "local_dataset = load_from_disk('demo/data/ChnSentiCorp')\n",
    "print(local_dataset)"
   ],
   "id": "6644c379356e8bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Dataset Format\n",
    "- Hugging Face的datasets库支持多种数据集格式，如CSV、JSON、TFRecord等。\n",
    "- 在本案例中，使用CSV格式，CSV文件应包含两列:一列是文本数据，另一列是情感标签。\n",
    "### 2.3 Dataset Inspection\n",
    "- 通过数据集的`features`属性可以查看数据集的特征信息。\n",
    "- 查看数据集的基本信息，如数据集大小、列名、数据示例等。"
   ],
   "id": "67e7ed98e125e1b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "remote_dataset = load_dataset(path=\"NousResearch/hermes-function-calling-v1\", split=\"train\")\n",
    "print(remote_dataset.features)\n",
    "remote_dataset_csv = remote_dataset.to_csv(path_or_buf='demo/data/hermes-function-calling-v1.csv')\n",
    "print(remote_dataset_csv)"
   ],
   "id": "ae1a7c1fba42c5f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3. Making Dataset\n",
    "- After loading the dataset, it needs to be processed to fit the input format of the model. This includes data cleaning, format conversion, etc.\n",
    "### 3.1 Dataset Column Selection\n",
    "When creating a dataset, you can select specific columns to include in the dataset. And each column should match the model's input and output format. In this case, we need to select the 'text' and 'label' columns.\n",
    "### 3.2 Dataset information\n",
    "After creating the dataset, you can check the dataset information by using `dataset.info`, such as the number of samples, column names, and data examples."
   ],
   "id": "7d1e4532f22a9539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dict_dataset = Dataset.from_dict({\n",
    "    \"text\": [\"I love Hugging Face\", \"I hate Hugging Face\"],\n",
    "    \"label\": [1, 0]\n",
    "})\n",
    "print(dict_dataset.info)\n",
    "\n",
    "dict_dataset.to_csv(path_or_buf='demo/data/dict_dataset.csv')\n",
    "dict_dataset_csv = load_dataset('csv', data_files='demo/data/dict_dataset.csv')\n",
    "print(dict_dataset_csv)"
   ],
   "id": "7c8a0e328f9db0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. vocab Dictionary\n",
    "- Before fine-tuning the BERT model, the model's vocabulary needs to be matched with the text in the dataset. This step ensures that the input text can be correctly converted into the model's input format.\n",
    "### 4.1 Vocabulary (vocab)\n",
    "- The BERT model uses a vocabulary (vocab) to convert text into the model's input format.\n",
    "- The vocabulary contains all the known words and their corresponding indices.\n",
    "- It is essential to ensure that all the text in the dataset can find the corresponding vocabulary index.\n",
    "### 4.2 Tokenization\n",
    "- The tokenizer is used to split the text into words in the vocabulary and convert them into the corresponding indices.\n",
    "- This step needs to ensure that the text length, special character processing, etc., are consistent with the BERT model's pre-training settings."
   ],
   "id": "757089eb3ab4f86f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "dataset_map = dict_dataset_csv.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True), batched=True)\n",
    "\n",
    "print(dataset_map)"
   ],
   "id": "b621f7e78e93bdac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Design Task-Specific Model\n",
    "- Before fine-tuning the BERT model, you need to design a downstream model structure that fits the sentiment analysis task.\n",
    "- This usually includes one or more fully connected layers to convert the feature vectors output by BERT into classification results.\n",
    "\n",
    "### 5.1 Model Structure\n",
    "- The downstream task model usually includes the following parts:\n",
    "    - BERT Model: Used to generate context feature vectors for the text.\n",
    "    - Dropout Layer: Used to prevent overfitting by randomly dropping some neurons to improve the model's generalization ability.\n",
    "    - Fully Connected Layer: Used to map the output feature vectors of BERT to specific classification tasks.\n",
    "### 5.2 Model Initialization\n",
    "- The model is initialized by loading the pre-trained BERT model using the `BertModel.from_pretrained` method and initializing custom fully connected layers.\n",
    "- When initializing the model, you need to define the appropriate output dimensions based on the downstream task requirements."
   ],
   "id": "e3574c4d4bfde105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(768, 2) #假设情感分类为二分类任务 0: negative, 1: positive\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pool_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.dropout(pool_output)\n",
    "        return self.linear(output)"
   ],
   "id": "2dafea7da14eb4bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Customized Training Loop\n",
    "- After the model design is completed, enter the training phase. Batch processing data efficiently using DataLoader and updating model parameters using the optimizer.\n",
    "\n",
    "### 6.1 DataLoader\n",
    "Use DataLoader to implement batch data loading. DataLoader automatically handles batch processing and random shuffling of data to ensure training efficiency and data diversity.\n",
    "### 6.2 Optimizer\n",
    "AdamW is an optimizer suitable for BERT models. It combines the characteristics of Adam and weight decay to effectively prevent overfitting.\n",
    "### 6.3 Training Loop\n",
    "The training loop includes forward pass, loss calculation, backward pass, parameter update, etc. Each epoch traverses the entire dataset once, updating the model parameters. The loss value is usually tracked during training to determine the model's convergence.\n"
   ],
   "id": "6e5e295a974629f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Initialize the dataset\n",
    "my_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": dataset_map['train']['input_ids'],\n",
    "    \"attention_mask\": dataset_map['train']['attention_mask'],\n",
    "    \"labels\": dict_dataset_csv['train']['label']\n",
    "})\n",
    "print(my_dataset)\n",
    "\n",
    "# Initialize the data loader\n",
    "data_loader = DataLoader(my_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = SentimentAnalysisModel()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = nn.CrossEntropyLoss()(output, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ],
   "id": "31f87d16335cd9f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Evaluation\n",
    "- After training the model, evaluate its performance on the test set. Common metrics include accuracy, precision, recall, F1 score, etc.\n",
    "\n",
    "### 7.1 Accuracy\n",
    "- Accuracy is a basic metric for measuring the overall performance of a classification model.\n",
    "- It is calculated by dividing the number of correctly classified samples by the total number of samples."
   ],
   "id": "e186f50a11665c6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset_test = Dataset.from_dict({\n",
    "    \"input_ids\": dataset_map['test']['input_ids'],\n",
    "    \"attention_mask\": dataset_map['test']['attention_mask'],\n",
    "    \"labels\": dict_dataset_csv['test']['label']\n",
    "})\n",
    "\n",
    "# Initialize the test data loader\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "accuracy_score = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy_score:.4f}')"
   ],
   "id": "b16b9228afd04da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.2 Precision, Recall, and F1 Score\n",
    "- Precision and recall are two other important metrics for classification models, reflecting the model's accuracy and recall on positive predictions.\n",
    "- F1 score is the harmonic mean of precision and recall, usually used for evaluating imbalanced datasets."
   ],
   "id": "633a97bdd35a47c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')"
   ],
   "id": "cfa9a00c969d230a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 7.3 Result Analysis and Model Optimization\n",
    "- By analyzing the results on the test set, you can identify the strengths and weaknesses of the model.\n",
    "- For example, if the F1 score is low, it may be due to an imbalanced dataset, leading to poor performance on some categories.\n",
    "- By adjusting hyperparameters, improving data preprocessing steps, or using more complex model structures, you can further improve model performance.\n",
    "\n",
    "### 7.4 Save and Load Model\n",
    "- To use the trained model in the future, save it as a file for later loading for inference or further fine-tuning."
   ],
   "id": "438edf24796c18f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'demo/model/sentiment_analysis_model.pth')\n",
    "\n",
    "# Load the model\n",
    "model = SentimentAnalysisModel()\n",
    "model.load_state_dict(torch.load('demo/model/sentiment_analysis_model.pth'))\n",
    "model.eval()"
   ],
   "id": "39b1ee257090e052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Summary\n",
    "- In this tutorial, we have detailed how to use Hugging Face's BERT model for fine-tuning training of Chinese sentiment analysis.\n",
    "- We explained the entire fine-tuning process step by step, including loading the dataset, creating the Dataset, vocabulary operations, model design, custom training, and final performance evaluation and testing.\n",
    "- Through this tutorial, you should be able to master the basic process of fine-tuning downstream tasks using pre-trained language models and apply it to practical NLP projects."
   ],
   "id": "7fbe2baeaf73a06e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
