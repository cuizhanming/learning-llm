{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:08.118914Z",
     "start_time": "2025-02-09T15:17:08.105144Z"
    }
   },
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:09.300605Z",
     "start_time": "2025-02-09T15:17:09.296556Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:11.578451Z",
     "start_time": "2025-02-09T15:17:11.575628Z"
    }
   },
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"pdf/metagpt.pdf\",\n",
    "    \"pdf/longlora.pdf\",\n",
    "    \"pdf/selfrag.pdf\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:20.623434Z",
     "start_time": "2025-02-09T15:17:13.519016Z"
    }
   },
   "source": [
    "from utils import get_doc_tools2\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools2(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdf/metagpt.pdf\n",
      "Getting tools for paper: pdf/longlora.pdf\n",
      "Getting tools for paper: pdf/selfrag.pdf\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:43.641589Z",
     "start_time": "2025-02-09T15:17:43.638923Z"
    }
   },
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "bff58c52",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:27.617032Z",
     "start_time": "2025-02-09T15:17:27.614746Z"
    }
   },
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:45.571544Z",
     "start_time": "2025-02-09T15:17:45.566941Z"
    }
   },
   "source": [
    "len(initial_tools)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:48.602317Z",
     "start_time": "2025-02-09T15:17:48.401960Z"
    }
   },
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:17:56.093440Z",
     "start_time": "2025-02-09T15:17:50.215485Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the experiments described in the context is the PG19 test split.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. The evaluation results indicate that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and show promising results on these large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:18:09.978365Z",
     "start_time": "2025-02-09T15:17:57.764738Z"
    }
   },
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms existing models on various tasks, demonstrating enhanced performance in terms of factuality, correctness, and citation accuracy. Additionally, Self-RAG evaluates text generation by assessing factual relevance, supportiveness, and overall utility of the generated content, ensuring that the output aligns with given instructions and evidence.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a framework that extends the context sizes of pre-trained large language models efficiently with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. The method allows for extending Llama2 7B to 100k context or Llama2 70B to 32k on a single 8Ã— A100 machine while retaining the original architectures of the models. Additionally, LongLoRA is compatible with most existing techniques like Flash-Attention2.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms existing models on various tasks, demonstrating enhanced performance in terms of factuality, correctness, and citation accuracy. Additionally, Self-RAG evaluates text generation by assessing factual relevance, supportiveness, and overall utility of the generated content, ensuring that the output aligns with given instructions and evidence.\n",
      "\n",
      "LongLoRA is a framework that extends the context sizes of pre-trained large language models efficiently with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. The method allows for extending Llama2 7B to 100k context or Llama2 70B to 32k on a single 8Ã— A100 machine while retaining the original architectures of the models. Additionally, LongLoRA is compatible with most existing techniques like Flash-Attention2.\n",
      "Self-RAG is a framework that improves the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It trains a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special tokens called reflection tokens. This framework outperforms existing models on various tasks, demonstrating enhanced performance in terms of factuality, correctness, and citation accuracy. Additionally, Self-RAG evaluates text generation by assessing factual relevance, supportiveness, and overall utility of the generated content, ensuring that the output aligns with given instructions and evidence.\n",
      "\n",
      "LongLoRA is a framework that extends the context sizes of pre-trained large language models efficiently with limited computation cost. It combines improved LoRA with shifted sparse attention to achieve strong empirical results on various tasks with Llama2 models. The method allows for extending Llama2 7B to 100k context or Llama2 70B to 32k on a single 8Ã— A100 machine while retaining the original architectures of the models. Additionally, LongLoRA is compatible with most existing techniques like Flash-Attention2.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:18:23.343454Z",
     "start_time": "2025-02-09T15:18:23.340790Z"
    }
   },
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"pdf/metagpt.pdf\",\n",
    "    \"pdf/longlora.pdf\",\n",
    "    \"pdf/loftq.pdf\",\n",
    "    \"pdf/swebench.pdf\",\n",
    "    \"pdf/selfrag.pdf\",\n",
    "    \"pdf/zipformer.pdf\",\n",
    "    \"pdf/values.pdf\",\n",
    "    \"pdf/finetune_fair_diffusion.pdf\",\n",
    "    \"pdf/knowledge_card.pdf\",\n",
    "    \"pdf/metra.pdf\",\n",
    "    \"pdf/vr_mcl.pdf\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:18:49.004841Z",
     "start_time": "2025-02-09T15:18:27.335026Z"
    }
   },
   "source": [
    "from utils import get_doc_tools2\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools2(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdf/metagpt.pdf\n",
      "Getting tools for paper: pdf/longlora.pdf\n",
      "Getting tools for paper: pdf/loftq.pdf\n",
      "Getting tools for paper: pdf/swebench.pdf\n",
      "Getting tools for paper: pdf/selfrag.pdf\n",
      "Getting tools for paper: pdf/zipformer.pdf\n",
      "Getting tools for paper: pdf/values.pdf\n",
      "Getting tools for paper: pdf/finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: pdf/knowledge_card.pdf\n",
      "Getting tools for paper: pdf/metra.pdf\n",
      "Getting tools for paper: pdf/vr_mcl.pdf\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:07.836689Z",
     "start_time": "2025-02-09T15:19:07.834018Z"
    }
   },
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:10.382428Z",
     "start_time": "2025-02-09T15:19:09.402620Z"
    }
   },
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:11.940930Z",
     "start_time": "2025-02-09T15:19:11.938275Z"
    }
   },
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:14.021791Z",
     "start_time": "2025-02-09T15:19:13.682953Z"
    }
   },
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:15.667725Z",
     "start_time": "2025-02-09T15:19:15.664356Z"
    }
   },
   "source": [
    "tools[2].metadata"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:17.287175Z",
     "start_time": "2025-02-09T15:19:17.284565Z"
    }
   },
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:37.347078Z",
     "start_time": "2025-02-09T15:19:19.909272Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in the study includes two public benchmarks, HumanEval and MBPP, along with a self-generated, more challenging software development benchmark named SoftwareDev. The HumanEval benchmark consists of 164 handwritten programming tasks, while the MBPP benchmark comprises 427 Python tasks. The SoftwareDev dataset contains 70 representative examples of software development tasks covering diverse scopes such as mini-games, image processing algorithms, and data visualization. These datasets serve as a robust testbed for evaluating the performance of MetaGPT in software development tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset comprises task instances sourced from various open-source repositories, each presenting specific problems and corresponding codebase changes. It is used to assess different models' performance in generating patches to resolve the stated issues. The dataset includes details on files edited, lines added and removed, functions modified, and test results to evaluate the effectiveness of the generated patches. Task instances are drawn from repositories like scikit-learn, xarray, django, sphinx-doc, among others, with each instance highlighting a particular issue or bug within the codebase. Models such as SWE-Llama 13b and Claude 2 are responsible for creating patches based on the issue description, steps to reproduce, and expected versus actual results. The success of these models in producing accurate patches varies across the task instances, with some models successfully addressing the issues while others do not meet the mark. This dataset acts as a benchmark to gauge the performance and capabilities of these models in tackling real-world software engineering challenges.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes two public benchmarks, HumanEval and MBPP, along with a self-generated benchmark named SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev contains 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization. These datasets are used to evaluate MetaGPT's performance in software development tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset in SWE-Bench comprises task instances sourced from open-source repositories, each presenting specific problems and corresponding codebase changes. It is used to assess models' performance in generating patches to resolve the stated issues. The dataset includes details on files edited, lines added and removed, functions modified, and test results to evaluate the effectiveness of the generated patches. Task instances are drawn from repositories like scikit-learn, xarray, django, sphinx-doc, among others. Models such as SWE-Llama 13b and Claude 2 create patches based on issue descriptions, steps to reproduce, and expected versus actual results. The success of these models in producing accurate patches varies across task instances, with some models successfully addressing the issues while others do not meet the mark. This dataset serves as a benchmark to assess the performance and capabilities of these models in tackling real-world software engineering challenges.\n",
      "The evaluation dataset used in MetaGPT includes two public benchmarks, HumanEval and MBPP, along with a self-generated benchmark named SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev contains 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization. These datasets are used to evaluate MetaGPT's performance in software development tasks.\n",
      "\n",
      "On the other hand, the evaluation dataset in SWE-Bench comprises task instances sourced from open-source repositories, each presenting specific problems and corresponding codebase changes. It is used to assess models' performance in generating patches to resolve the stated issues. The dataset includes details on files edited, lines added and removed, functions modified, and test results to evaluate the effectiveness of the generated patches. Task instances are drawn from repositories like scikit-learn, xarray, django, sphinx-doc, among others. Models such as SWE-Llama 13b and Claude 2 create patches based on issue descriptions, steps to reproduce, and expected versus actual results. The success of these models in producing accurate patches varies across task instances, with some models successfully addressing the issues while others do not meet the mark. This dataset serves as a benchmark to assess the performance and capabilities of these models in tackling real-world software engineering challenges.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T15:19:54.297800Z",
     "start_time": "2025-02-09T15:19:41.425885Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"Analyzing the approach in the LongLoRA paper.\"}\n",
      "=== Function Output ===\n",
      "The approach introduced in the LongLoRA paper focuses on enhancing the context length of large language models by incorporating shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns. This method enables extending the context window of models like Llama2 7B and 13B to significantly larger lengths, such as 100k and 32k respectively, on a single 8Ã— A100 machine. The approach maintains the original attention architecture during inference, ensuring compatibility with existing optimization and infrastructure. Furthermore, the paper introduces a supervised fine-tuning solution using the LongAlpaca dataset to enhance chat ability in large language models. Additionally, the LongLoRA paper introduces the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) to improve forgery detection by capturing intra-face relations and generating challenging pseudosamples for model learning. This framework achieves state-of-the-art performance on cross-dataset evaluations and emphasizes the significance of modeling relations between different facial action units to enhance generalization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"Analyzing the approach in the LoftQ paper.\"}\n",
      "=== Function Output ===\n",
      "The approach introduced in the LoftQ paper focuses on a novel quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This method aims to provide a beneficial initialization for subsequent LoRA fine-tuning, addressing performance gaps observed in downstream tasks when quantization and fine-tuning are combined. LoftQ has been demonstrated to outperform existing quantization methods, particularly in challenging low-bit scenarios, across various tasks in natural language processing. By utilizing low-rank adapters, LoftQ effectively reduces memory requirements during training and storage, showcasing its potential for model compression and task adaptation in large language models. The method also involves setting specific hyperparameters for different tasks and datasets, and it has shown superior performance in reducing memory usage compared to pruning methods. Additionally, the paper extends the application of low-rank adapters to convolutional layers, highlighting the versatility of LoftQ in various scenarios for efficient model compression.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an approach that enhances the context length of large language models by incorporating shifted sparse attention (S2-Attn) during training. This method allows for extending the context window of models like Llama2 7B and 13B to significantly larger lengths on a single machine. The paper also presents a supervised fine-tuning solution using the LongAlpaca dataset to improve chat ability in large language models. Additionally, LongLoRA introduces the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP) for forgery detection, achieving state-of-the-art performance on cross-dataset evaluations.\n",
      "\n",
      "On the other hand, the LoftQ paper focuses on a quantization framework that combines quantization and low-rank approximation to approximate high-precision pre-trained weights. This framework aims to provide a beneficial initialization for subsequent LoRA fine-tuning, addressing performance gaps observed in downstream tasks. LoftQ outperforms existing quantization methods, especially in low-bit scenarios, across various natural language processing tasks. It effectively reduces memory requirements during training and storage, showcasing potential for model compression and task adaptation in large language models. LoftQ also extends the application of low-rank adapters to convolutional layers, demonstrating versatility in efficient model compression.\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
