{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning BERT model for news\n",
    "\n",
    "## 1. Download Pre-trained Model\n",
    "- Download the pre-trained BERT model from the Hugging Face model hub.\n",
    "- We will do fine-tuning on top of it for the sentiment analysis task."
   ],
   "id": "e80c5bdd659f396a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"google-bert/bert-base-chinese\"\n",
    "cache_dir = \"../local_models\"\n",
    "AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ],
   "id": "c337dd2b14040467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Define Dataset class, for loading our custom dataset for further fine-tuning\n",
    "- The dataset should be prepared in advance, and in different purpose, like train, validation, test, etc.\n",
    "- There should more data for training, smaller size for validation and test. like 80%, 10%, 10%."
   ],
   "id": "2a9ec39572186a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(path=\"csv\", data_files=f\"../local_datasets/news/{split}.csv\", split=\"train\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.dataset[item]['text']\n",
    "        label = self.dataset[item]['label']\n",
    "        return text,label\n",
    "\n",
    "dataset_train = MyDataset(\"train\")\n",
    "for data in dataset_train[:5]:\n",
    "    print(data)\n",
    "\n",
    "dataset_validation = MyDataset(\"validation\")\n",
    "for data in dataset_validation[:5]:\n",
    "    print(data)\n",
    "\n",
    "dataset_test = MyDataset(\"test\")\n",
    "for data in dataset_test[:5]:\n",
    "    print(data)"
   ],
   "id": "6644c379356e8bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Define downstream tasks model\n",
    "- Extending the pretrained model for the fine-tuning;\n",
    "- Update model config to support larger input size;\n",
    "- default BERT model size was 512, we need to increase it to 1024."
   ],
   "id": "67e7ed98e125e1b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertModel,BertConfig\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "model_path = r\"../local_models/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\"\n",
    "# config to support larger input size\n",
    "config = BertConfig.from_pretrained(model_path)\n",
    "config.max_position_embeddings = 1024\n",
    "\n",
    "# Initialize the model with the config\n",
    "pretrained_model = BertModel(config).to(DEVICE)\n",
    "print(pretrained_model)\n",
    "\n",
    "# Define the downstream task model, with more classification layers\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 10)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        # As result of config change, we need to do full training\n",
    "        out = pretrained_model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        # Incremental model\n",
    "        out = self.fc(out.last_hidden_state[:,0])\n",
    "        return out"
   ],
   "id": "ae1a7c1fba42c5f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 4. Training\n"
   ],
   "id": "7d1e4532f22a9539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer,AdamW\n",
    "import torch\n",
    "\n",
    "# Previous step loaded the DEVICE already, otherwise, you can load it again.\n",
    "DEVICE = torch.device(\"cuba\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer to encode the data\n",
    "model_path = r\"../local_models/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# to encode the data while loading process\n",
    "def tokenize_batches(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=texts,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        return_length=True\n",
    "    )\n",
    "    tensor_labels = torch.tensor(labels)\n",
    "    return encoded_data[\"input_ids\"], encoded_data[\"attention_mask\"], encoded_data[\"token_type_ids\"], tensor_labels\n",
    "\n",
    "# loading the training dataset\n",
    "dataset_train = MyDataset(\"train\")\n",
    "train_data_loader = DataLoader (\n",
    "    dataset_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "# loading the validation dataset\n",
    "dataset_train = MyDataset(\"validation\")\n",
    "validation_data_loader = DataLoader (\n",
    "    dataset_validation,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "EPOCH = 2 # This should be very large, like 30000, but for the demo, we set it to 3.\n",
    "def run_training(data_loader=train_data_loader):\n",
    "    print(DEVICE)\n",
    "    model = MyModel().to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_validation_acc = 0.0\n",
    "    for epoch in range(EPOCH):\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(data_loader):\n",
    "            input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_func(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out==labels).sum().item()/len(labels)\n",
    "                print(f\"epoch:{epoch},i:{i},loss:{loss.item()},acc:{acc}\")\n",
    "\n",
    "        model.eval()\n",
    "        # Validation doesn't need engaging the pretrained model\n",
    "        with torch.no_grad():\n",
    "            validation_acc = 0.0\n",
    "            validation_loss = 0.0\n",
    "            for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(validation_data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "                out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                validation_loss += loss_func(out, labels)\n",
    "                out = out.argmax(dim=1)\n",
    "                validation_acc += (out==labels).sum().item()\n",
    "            validation_loss /= len(data_loader)\n",
    "            validation_acc /= len(data_loader)\n",
    "            print(f\"epoch:{epoch},validation_loss:{validation_loss},validation_acc:{validation_acc}\")\n",
    "\n",
    "            if validation_acc > best_validation_acc:\n",
    "                best_validation_acc = validation_acc\n",
    "                torch.save(model.state_dict(), \"params/best.pth\")\n",
    "                print(f\"epoch:{epoch},best model saved with acc:{best_validation_acc}\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"params/last.pth\")\n",
    "        print(f\"epoch:{epoch},last model saved\")\n",
    "\n",
    "run_training()"
   ],
   "id": "7c8a0e328f9db0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Testing\n",
    "- After training, we need to test the model on the test dataset.\n",
    "- Load the generated parameters model and test it"
   ],
   "id": "757089eb3ab4f86f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "dataset_test = MyDataset(\"test\")\n",
    "test_data_loader = DataLoader (\n",
    "    dataset_test,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "def run_testing(param_path=\"params/best.pth\"):\n",
    "    test_acc = 0.0\n",
    "    total = 0\n",
    "    model = MyModel().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(param_path))\n",
    "    model.eval()\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(test_data_loader):\n",
    "        input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        out = out.argmax(dim=1)\n",
    "        test_acc += (out==labels).sum().item()\n",
    "        print(i, (out==labels).sum().item())\n",
    "        total += len(labels)\n",
    "    print(f\"test_acc:{test_acc/total}\")\n",
    "\n",
    "run_testing()\n",
    "run_testing(\"params/last.pth\")"
   ],
   "id": "b621f7e78e93bdac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
