{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Using `LLaMA-Factory CLI`, to merge base model with checkpoint(adapter)\n",
    "\n",
    ">Note: DO NOT use quantized model or quantization_bit when merging lora adapters\n",
    "Create a file named `config.yaml` with the following content:\n",
    "```bash\n",
    "### model\n",
    "model_name_or_path: /root/autodl-tmp/models/LLM-Research/Llama-3.2-1B-Instruct/\n",
    "adapter_name_or_path: /root/autodl-tmp/LLaMA-Factory/saves/Llama-3.2-1B-Instruct/lora/train_2025-01-11-00-05-34\n",
    "template: llama3\n",
    "finetuning_type: lora\n",
    "### export\n",
    "export_dir: /root/autodl-tmp/models/LLM-Research/Llama-3.2-1B-Instruct-Lora-merged\n",
    "export_size: 4\n",
    "export_device: cuda\n",
    "export_legacy_format: false\n",
    "```\n",
    "\n",
    "Run the following command:\n",
    "```bash\n",
    "llamafactory-cli export /path/to/config.yaml\n",
    "```\n",
    "\n",
    "## 2. Using `llama.cpp` tool, to covert a HuggingFace model to GGUF format\n",
    "\n",
    "- Installing the tool, and convert the model to GGUF format\n",
    "```bash\n",
    "# download llama.cpp\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "# install requirements\n",
    "pip install -r llama.cpp/requirements.txt\n",
    "pip install sentencepiece\n",
    "pip install safetensors\n",
    "pip install transformers\n",
    "# convert model format\n",
    "python llama.cpp/convert_hf_to_gguf.py ./path/to/the/model/model_name --outtype f16 --verbose --outfile ./path/to/the/gguf/file/model_name.gguf\n",
    "```\n",
    "\n",
    "- Create a meta file for the model, to be used by OLlama inference framework\n",
    "create a file named `ModelFile` with the following content:\n",
    "```bash\n",
    "FROM /path/to/the/gguf/file/model-name.gguf\n",
    "```\n",
    "\n",
    "## 3. Using `OLlama` inference framework, to serve the model\n",
    "\n",
    "- Installing the tool, and serve the model\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "ollama serve\n",
    "ollama create model-name --file ./ModelFile\n",
    "ollama run model-name\n",
    "```\n",
    "\n",
    "## 4. Using `Open WebUI` to interact with the ollama served model\n",
    "\n",
    "- Installing the tool with a separate Python environment, and serve the model\n",
    "```bash\n",
    "source activate\n",
    "source deactivate\n",
    "conda create -n open-webui python==3.11\n",
    "conda init\n",
    "conda activate open-webui\n",
    "pip install -U open-webui torch transformers\n",
    "\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "export ENABLE_OLLAMA_API=True\n",
    "export OPENAI_API_BASE_URL=http://127.0.0.1:11434/v1\n",
    "export DEFAULT_MODELS=\"/path/to/the/model\"\n",
    "\n",
    "open-webui serve\n",
    "```"
   ],
   "id": "68061327f14d7d99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Logging\n",
    "\n",
    "```bash\n",
    "root@autodl-container-fb0c4680e1-702ff0ba:~/autodl-tmp# python llama.cpp/convert_hf_to_gguf.py ./models/LLM-Research/Llama-3.2-1B-Instruct-Lora-merged --outtype f16 --verbose --outfile ./models/LLM-Research/Llama-3.2-1B-Instruct-Lora-merged.gguf\n",
    "INFO:hf-to-gguf:Loading model: Llama-3.2-1B-Instruct-Lora-merged\n",
    "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
    "INFO:hf-to-gguf:Exporting model...\n",
    "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
    "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
    "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {2048, 128256}\n",
    "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {8192, 2048}\n",
    "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {2048, 8192}\n",
    "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {2048, 2048}\n",
    "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {2048, 512}\n",
    "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {2048}\n",
    "INFO:hf-to-gguf:Set meta model\n",
    "INFO:hf-to-gguf:Set model parameters\n",
    "INFO:hf-to-gguf:gguf: context length = 131072\n",
    "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
    "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
    "INFO:hf-to-gguf:gguf: head count = 32\n",
    "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
    "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
    "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
    "INFO:hf-to-gguf:gguf: file type = 1\n",
    "INFO:hf-to-gguf:Set model tokenizer\n",
    "DEBUG:hf-to-gguf:chktok: [128000, 198, 4815, 15073, 66597, 8004, 1602, 2355, 79772, 11187, 9468, 248, 222, 320, 8416, 8, 27623, 114, 102470, 9468, 234, 104, 31643, 320, 36773, 100166, 98634, 8, 26602, 227, 11410, 99, 247, 9468, 99, 247, 220, 18, 220, 1644, 220, 8765, 220, 8765, 18, 220, 8765, 1644, 220, 8765, 8765, 220, 8765, 8765, 18, 220, 8765, 8765, 1644, 220, 18, 13, 18, 220, 18, 497, 18, 220, 18, 1131, 18, 220, 21549, 222, 98629, 241, 45358, 233, 21549, 237, 45358, 224, 21549, 244, 21549, 115, 21549, 253, 45358, 223, 21549, 253, 21549, 95, 98629, 227, 76460, 223, 949, 37046, 101067, 19000, 23182, 102301, 9263, 18136, 16, 36827, 21909, 56560, 54337, 19175, 102118, 13373, 64571, 34694, 3114, 112203, 80112, 3436, 106451, 14196, 14196, 74694, 3089, 3089, 29249, 17523, 3001, 27708, 7801, 358, 3077, 1027, 364, 83, 820, 568, 596, 1070, 11, 364, 793, 499, 2771, 30, 364, 44, 539, 2771, 358, 3358, 1304, 433, 11, 364, 35, 499, 1093, 1063, 15600, 30, 1226, 6, 43712, 264, 64966, 43]\n",
    "DEBUG:hf-to-gguf:chkhsh: 0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\n",
    "DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'llama-bpe'\n",
    "DEBUG:hf-to-gguf:chkhsh: 0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5\n",
    "INFO:gguf.vocab:Adding 280147 merge(s).\n",
    "INFO:gguf.vocab:Setting special token type bos to 128000\n",
    "INFO:gguf.vocab:Setting special token type eos to 128009\n",
    "INFO:gguf.vocab:Setting special token type pad to 128009\n",
    "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
    "{%- if custom_tools is defined %}\n",
    "    {%- set tools = custom_tools %}\n",
    "{%- endif %}\n",
    "{%- if not tools_in_user_message is defined %}\n",
    "    {%- set tools_in_user_message = true %}\n",
    "{%- endif %}\n",
    "{%- if not date_string is defined %}\n",
    "    {%- if strftime_now is defined %}\n",
    "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
    "    {%- else %}\n",
    "        {%- set date_string = \"26 Jul 2024\" %}\n",
    "    {%- endif %}\n",
    "{%- endif %}\n",
    "{%- if not tools is defined %}\n",
    "    {%- set tools = none %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
    "{%- if messages[0]['role'] == 'system' %}\n",
    "    {%- set system_message = messages[0]['content']|trim %}\n",
    "    {%- set messages = messages[1:] %}\n",
    "{%- else %}\n",
    "    {%- set system_message = \"\" %}\n",
    "{%- endif %}\n",
    "\n",
    "{#- System message #}\n",
    "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
    "{%- if tools is not none %}\n",
    "    {{- \"Environment: ipython\\n\" }}\n",
    "{%- endif %}\n",
    "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
    "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
    "{%- if tools is not none and not tools_in_user_message %}\n",
    "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "{%- endif %}\n",
    "{{- system_message }}\n",
    "{{- \"<|eot_id|>\" }}\n",
    "\n",
    "{#- Custom tools are passed in a user message with some extra guidance #}\n",
    "{%- if tools_in_user_message and not tools is none %}\n",
    "    {#- Extract the first user message so we can plug it in here #}\n",
    "    {%- if messages | length != 0 %}\n",
    "        {%- set first_user_message = messages[0]['content']|trim %}\n",
    "        {%- set messages = messages[1:] %}\n",
    "    {%- else %}\n",
    "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
    "{%- endif %}\n",
    "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
    "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
    "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
    "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
    "    {{- \"Do not use variables.\\n\\n\" }}\n",
    "    {%- for t in tools %}\n",
    "        {{- t | tojson(indent=4) }}\n",
    "        {{- \"\\n\\n\" }}\n",
    "    {%- endfor %}\n",
    "    {{- first_user_message + \"<|eot_id|>\"}}\n",
    "{%- endif %}\n",
    "\n",
    "{%- for message in messages %}\n",
    "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
    "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
    "    {%- elif 'tool_calls' in message %}\n",
    "        {%- if not message.tool_calls|length == 1 %}\n",
    "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
    "        {%- endif %}\n",
    "        {%- set tool_call = message.tool_calls[0].function %}\n",
    "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
    "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
    "        {{- '\"parameters\": ' }}\n",
    "        {{- tool_call.arguments | tojson }}\n",
    "        {{- \"}\" }}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
    "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
    "        {%- if message.content is mapping or message.content is iterable %}\n",
    "            {{- message.content | tojson }}\n",
    "        {%- else %}\n",
    "            {{- message.content }}\n",
    "        {%- endif %}\n",
    "        {{- \"<|eot_id|>\" }}\n",
    "    {%- endif %}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
    "{%- endif %}\n",
    "\n",
    "INFO:hf-to-gguf:Set model quantization version\n",
    "INFO:gguf.gguf_writer:Writing the following files:\n",
    "INFO:gguf.gguf_writer:models/LLM-Research/Llama-3.2-1B-Instruct-Lora-merged.gguf: n_tensors = 147, total_size = 2.5G\n",
    "Writing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.47G/2.47G [00:06<00:00, 394Mbyte/s]\n",
    "INFO:hf-to-gguf:Model successfully exported to models/LLM-Research/Llama-3.2-1B-Instruct-Lora-merged.gguf\n",
    "```"
   ],
   "id": "6fb39bb731500fbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
