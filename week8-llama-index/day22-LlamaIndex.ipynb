{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LlamaIndex framework\n",
    ">LlamaIndex is the leading data framework for building LLM applications\n",
    "\n",
    "## 1. Installation\n",
    "\n",
    "### Default installation for OpenAI integration\n",
    "\n",
    "```bash\n",
    "pip install llama-index\n",
    "```\n",
    "Which includes the following packages:\n",
    "- llama-index-core\n",
    "- llama-index-llms-openai\n",
    "- llama-index-embeddings-openai\n",
    "- llama-index-program-openai\n",
    "- llama-index-question-gen-openai\n",
    "- llama-index-agent-openai\n",
    "- llama-index-readers-file\n",
    "- llama-index-multi-modal-llms-openai\n",
    "\n",
    "**LLAMA_INDEX_CACHE_DIR** - Environment variable to set the cache directory\n",
    "\n",
    "### Custom installation for local LLMs\n",
    "\n",
    "```bash\n",
    "pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
    "```\n",
    "Which can use local LLMs from Ollama, and indexing with Huggingface\n",
    "\n",
    "## 2. Load Data and build an index"
   ],
   "id": "9ba6bf15a56e7fd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install llama-index\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load data from the directory\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "print(documents)\n",
    "# Load specific file type, i.e. \".pdf\"\n",
    "documents1 = SimpleDirectoryReader(\"data\", required_exts=[\".pdf\"]).load_data()\n",
    "print(documents1)\n",
    "# Load specific files, which `input_files` will override `input_dir`.\n",
    "documents2 = SimpleDirectoryReader(input_dir=\"data\", input_files=[\"/path/to/data/requirements.txt\"]).load_data()\n",
    "print(documents2)\n",
    "\n",
    "# Build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(index)"
   ],
   "id": "87d51365f63016c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Query your data\n",
    "\n"
   ],
   "id": "a672bd547c8c189c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"`688396.SH`是哪家公司?\")\n",
    "print(response)"
   ],
   "id": "26c540885b3925d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Logging - Queries and Events",
   "id": "3b109e77c3887e00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging, sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARN)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ],
   "id": "6f3fe3522428dcd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Storing your index\n",
    "\n",
    "By default, the data you just loaded is stored in memory as a series of vector embeddings.\n",
    "You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:"
   ],
   "id": "be12fe46117c1205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "index.storage_context.persist()",
   "id": "3d7064c251478030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Of course, you don't get the benefits of persisting unless you load the data. So let's modify the code to generate and store the index if it doesn't exist, but load it if it does:",
   "id": "bca43618c4869554"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "print(os.path.getsize(PERSIST_DIR))\n",
    "\n",
    "if os.path.exists(PERSIST_DIR) and os.path.isdir(PERSIST_DIR) and any(os.scandir(PERSIST_DIR)):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"`688396.SH`是哪家公司?\")\n",
    "print(response)"
   ],
   "id": "69601a0bd27cd27d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Using local LLM instead of OpenAI API\n",
    "\n",
    "### Start Ollama serve with a chosen LLM model\n",
    "\n",
    "```bash\n",
    "# Run Ollama serve\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "# Exec a given LLM model\n",
    "docker exec -it ollama ollama run deepseek-r1:7b\n",
    "```\n",
    "\n",
    "### Load data and build an index with local LLM, and Query"
   ],
   "id": "f90135f19023f5b2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-05T10:06:41.605260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "Settings.embed_model = HuggingFaceEmbedding(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "Settings.llm = Ollama(\"deepseek-r1:7b\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"`688396.SH`是哪家公司?\")\n",
    "print(response)"
   ],
   "id": "4be0475a3b992f71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.readers.file.base:> [SimpleDirectoryReader] Total files added: 3\n",
      "> [SimpleDirectoryReader] Total files added: 3\n",
      "> [SimpleDirectoryReader] Total files added: 3\n",
      "DEBUG:fsspec.local:open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/README_zh-CN.md\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/README_zh-CN.md\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/README_zh-CN.md\n",
      "DEBUG:fsspec.local:open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/pdf内容研报.pdf\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/pdf内容研报.pdf\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/pdf内容研报.pdf\n",
      "DEBUG:fsspec.local:open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/requirements.txt\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/requirements.txt\n",
      "open file: /Users/zcui/Workspace/github/llm-projects/week8-llama-index/data/requirements.txt\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "Load pretrained SentenceTransformer: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "Load pretrained SentenceTransformer: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/modules.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/modules.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/modules.json HTTP/1.1\" 404 0\n",
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name deepseek-ai/DeepSeek-R1-Distill-Qwen-7B. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name deepseek-ai/DeepSeek-R1-Distill-Qwen-7B. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name deepseek-ai/DeepSeek-R1-Distill-Qwen-7B. Creating a new one with mean pooling.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "142c05738d0c4c4982660ec41ef3925c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
