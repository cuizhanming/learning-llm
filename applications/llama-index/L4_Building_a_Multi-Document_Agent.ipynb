{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b523e0a",
   "metadata": {},
   "source": [
    "# Lesson 4: Building a Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323703",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:20.348518Z",
     "start_time": "2025-02-09T13:39:20.340487Z"
    }
   },
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:20.997766Z",
     "start_time": "2025-02-09T13:39:20.994295Z"
    }
   },
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "20adaa26",
   "metadata": {},
   "source": [
    "## 1. Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b71ff6",
   "metadata": {},
   "source": [
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:23.299593Z",
     "start_time": "2025-02-09T13:39:23.297562Z"
    }
   },
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"pdf/metagpt.pdf\",\n",
    "    \"pdf/longlora.pdf\",\n",
    "    \"pdf/selfrag.pdf\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:35.019267Z",
     "start_time": "2025-02-09T13:39:25.693279Z"
    }
   },
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdf/metagpt.pdf\n",
      "Getting tools for paper: pdf/longlora.pdf\n",
      "Getting tools for paper: pdf/selfrag.pdf\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:37.227221Z",
     "start_time": "2025-02-09T13:39:37.225525Z"
    }
   },
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bff58c52",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:40.680775Z",
     "start_time": "2025-02-09T13:39:40.678458Z"
    }
   },
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:43.414409Z",
     "start_time": "2025-02-09T13:39:43.409090Z"
    }
   },
   "source": [
    "len(initial_tools)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:39:45.935990Z",
     "start_time": "2025-02-09T13:39:45.763107Z"
    }
   },
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:40:00.142925Z",
     "start_time": "2025-02-09T13:39:50.815384Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset used in LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results of LongLoRA\"}\n",
      "=== Function Output ===\n",
      "The evaluation results of LongLoRA demonstrate comparable or superior performance to other Llama2-based long-context models across different benchmarks. LongLoRA showcases efficiency in terms of training hours and GPU memory usage, presenting significant enhancements in training speed and memory efficiency compared to full fine-tuning. Additionally, the incorporation of the S2-Attn mechanism in LongLoRA reduces FLOPs and training hours, making it a more efficient choice for extending context lengths.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "The evaluation results of LongLoRA demonstrate comparable or superior performance to other Llama2-based long-context models across different benchmarks. LongLoRA showcases efficiency in terms of training hours and GPU memory usage, presenting significant enhancements in training speed and memory efficiency compared to full fine-tuning. Additionally, the incorporation of the S2-Attn mechanism in LongLoRA reduces FLOPs and training hours, making it a more efficient choice for extending context lengths.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:40:17.962908Z",
     "start_time": "2025-02-09T13:40:04.645916Z"
    }
   },
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It involves training a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating its effectiveness in improving generation quality, factuality, and citation accuracy. Additionally, Self-RAG evaluates the factual accuracy and relevance of generated text by training a Critic LM to predict the necessity of retrieval for generating responses and a Generator LM to produce responses based on the predicted reflection tokens.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a versatile approach that enhances the context length of Large Language Models (LLMs) efficiently, while reducing GPU memory usage and training time compared to standard full fine-tuning. It introduces S2-Attn during training to approximate the standard self-attention pattern. LongLoRA maintains the original attention architecture during inference, ensuring compatibility with existing infrastructure and optimization techniques. By enabling trainable normalization and embedding layers, LongLoRA bridges the gap between LoRA and full fine-tuning. Additionally, it has demonstrated the ability to extend the context length of Llama2 7B and 70B models significantly on a single 8Ã— A100 machine.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It involves training a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating its effectiveness in improving generation quality, factuality, and citation accuracy. Additionally, Self-RAG evaluates the factual accuracy and relevance of generated text by training a Critic LM to predict the necessity of retrieval for generating responses and a Generator LM to produce responses based on the predicted reflection tokens.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is a versatile approach that enhances the context length of Large Language Models (LLMs) efficiently, while reducing GPU memory usage and training time compared to standard full fine-tuning. It introduces S2-Attn during training to approximate the standard self-attention pattern. LongLoRA maintains the original attention architecture during inference, ensuring compatibility with existing infrastructure and optimization techniques. By enabling trainable normalization and embedding layers, LongLoRA bridges the gap between LoRA and full fine-tuning. Additionally, it has demonstrated the ability to extend the context length of Llama2 7B and 70B models significantly on a single 8Ã— A100 machine.\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval on demand and self-reflection. It involves training a single arbitrary LM to adaptively retrieve passages, generate text informed by these passages, and critique its own output using special reflection tokens. This framework significantly outperforms other LLMs and retrieval-augmented models on various tasks, demonstrating its effectiveness in improving generation quality, factuality, and citation accuracy. Additionally, Self-RAG evaluates the factual accuracy and relevance of generated text by training a Critic LM to predict the necessity of retrieval for generating responses and a Generator LM to produce responses based on the predicted reflection tokens.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is a versatile approach that enhances the context length of Large Language Models (LLMs) efficiently, while reducing GPU memory usage and training time compared to standard full fine-tuning. It introduces S2-Attn during training to approximate the standard self-attention pattern. LongLoRA maintains the original attention architecture during inference, ensuring compatibility with existing infrastructure and optimization techniques. By enabling trainable normalization and embedding layers, LongLoRA bridges the gap between LoRA and full fine-tuning. Additionally, it has demonstrated the ability to extend the context length of Llama2 7B and 70B models significantly on a single 8Ã— A100 machine.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "7eede70c",
   "metadata": {},
   "source": [
    "## 2. Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18771e69",
   "metadata": {},
   "source": [
    "### Download 11 ICLR papers"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
   "metadata": {
    "height": 472,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:41:07.050237Z",
     "start_time": "2025-02-09T13:41:07.046810Z"
    }
   },
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"pdf/metagpt.pdf\",\n",
    "    \"pdf/longlora.pdf\",\n",
    "    \"pdf/loftq.pdf\",\n",
    "    \"pdf/swebench.pdf\",\n",
    "    \"pdf/selfrag.pdf\",\n",
    "    \"pdf/zipformer.pdf\",\n",
    "    \"pdf/values.pdf\",\n",
    "    \"pdf/finetune_fair_diffusion.pdf\",\n",
    "    \"pdf/knowledge_card.pdf\",\n",
    "    \"pdf/metra.pdf\",\n",
    "    \"pdf/vr_mcl.pdf\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "b77426cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download these papers, below is the needed code:\n",
    "\n",
    "\n",
    "    #for url, paper in zip(urls, papers):\n",
    "         #!wget \"{url}\" -O \"{paper}\"\n",
    "    \n",
    "    \n",
    "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:41:31.233766Z",
     "start_time": "2025-02-09T13:41:09.700569Z"
    }
   },
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: pdf/metagpt.pdf\n",
      "Getting tools for paper: pdf/longlora.pdf\n",
      "Getting tools for paper: pdf/loftq.pdf\n",
      "Getting tools for paper: pdf/swebench.pdf\n",
      "Getting tools for paper: pdf/selfrag.pdf\n",
      "Getting tools for paper: pdf/zipformer.pdf\n",
      "Getting tools for paper: pdf/values.pdf\n",
      "Getting tools for paper: pdf/finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: pdf/knowledge_card.pdf\n",
      "Getting tools for paper: pdf/metra.pdf\n",
      "Getting tools for paper: pdf/vr_mcl.pdf\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "4e35d52c",
   "metadata": {},
   "source": [
    "### Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:41:38.801464Z",
     "start_time": "2025-02-09T13:41:38.799212Z"
    }
   },
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
   "metadata": {
    "height": 149,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:41:45.740987Z",
     "start_time": "2025-02-09T13:41:45.273496Z"
    }
   },
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:42:02.602634Z",
     "start_time": "2025-02-09T13:42:02.599922Z"
    }
   },
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
   "metadata": {
    "height": 64,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:42:40.118010Z",
     "start_time": "2025-02-09T13:42:39.390804Z"
    }
   },
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
   "metadata": {
    "height": 30,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:42:42.455965Z",
     "start_time": "2025-02-09T13:42:42.452923Z"
    }
   },
   "source": [
    "tools[2].metadata"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_finetune_fair_diffusion', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
   "metadata": {
    "height": 251,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:42:50.049183Z",
     "start_time": "2025-02-09T13:42:50.046380Z"
    }
   },
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:43:03.062673Z",
     "start_time": "2025-02-09T13:42:54.124104Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues in the repository and introduce new tests. Each task instance consists of a codebase snapshot, a description of the issue to be resolved, and the associated pull request's code changes. The dataset is continuously updated to include new task instances from popular repositories, ensuring a diverse and challenging set of problems for language models to solve.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues in the repository and introduce new tests. Each task instance consists of a codebase snapshot, a description of the issue to be resolved, and the associated pull request's code changes. The dataset is continuously updated to include new task instances from popular repositories, ensuring a diverse and challenging set of problems for language models to solve.\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench is constructed by scraping pull requests from the top 100 most downloaded PyPI libraries. Task instances are created from merged pull requests that resolve issues in the repository and introduce new tests. Each task instance consists of a codebase snapshot, a description of the issue to be resolved, and the associated pull request's code changes. The dataset is continuously updated to include new task instances from popular repositories, ensuring a diverse and challenging set of problems for language models to solve.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-02-09T13:43:42.995382Z",
     "start_time": "2025-02-09T13:43:32.057368Z"
    }
   },
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n",
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach for extending the context length of Large Language Models (LLMs) using Shifted Sparse Attention (S2-Attn) to approximate standard self-attention patterns during training. It incorporates trainable normalization and embedding layers to bridge the gap between Low-rank Adaptation (LoRA) and full fine-tuning, showcasing improved performance in extending context lengths for Llama2 models with minimal accuracy compromise. Additionally, the paper introduces an Action Units Relation Learning framework comprising the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components for forgery detection, achieving state-of-the-art performance on cross-dataset and cross-manipulation evaluations.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a novel quantization framework introduced in a paper presented at ICLR 2024. It focuses on applying quantization and low-rank approximation techniques to pre-trained weights of Large Language Models (LLMs). LoftQ provides an advantageous initialization for subsequent LoRA fine-tuning, leading to improved generalization in downstream tasks. This method has been shown to outperform existing quantization methods, especially in challenging low-bit quantization scenarios, effectively compressing models while maintaining performance.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning approach for extending the context length of Large Language Models (LLMs) using Shifted Sparse Attention (S2-Attn) to approximate standard self-attention patterns during training. It incorporates trainable normalization and embedding layers to bridge the gap between Low-rank Adaptation (LoRA) and full fine-tuning, showcasing improved performance in extending context lengths for Llama2 models with minimal accuracy compromise. Additionally, the paper introduces an Action Units Relation Learning framework comprising the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP) components for forgery detection, achieving state-of-the-art performance on cross-dataset and cross-manipulation evaluations.\n",
      "\n",
      "On the other hand, LoftQ is a novel quantization framework introduced in a paper presented at ICLR 2024. It focuses on applying quantization and low-rank approximation techniques to pre-trained weights of Large Language Models (LLMs). LoftQ provides an advantageous initialization for subsequent LoRA fine-tuning, leading to improved generalization in downstream tasks. This method has been shown to outperform existing quantization methods, especially in challenging low-bit quantization scenarios, effectively compressing models while maintaining performance.\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
