{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. LLaMA-Factory `Evaluation` Tab\n",
    "\n",
    "## 1.1 Understand Evaluation Output\n",
    "[CSDN blog](https://blog.csdn.net/weixin_45573296/article/details/141333719)\n",
    "- BLEU-4: BLEU分数：评价一种语言翻译成另一种语言的文本质量的指标。BLEU分数越高，表示翻译质量越高。\n",
    "- ROUGE: 指标是在机器翻译、自动摘要、问答生成等领域常见的评估指标。\n",
    "ROUGE+通过将模型生成的摘要或者回答与参考答案(一般是人工生成的)进行比较计算，得到对应的得分。\n",
    "ROUGE指标与BLEU指标非常类似，均可用来衡量生成结果和标准结果的匹配程度，不同的是ROUGE基于`召回率`，BLEU更看重`准确率`。\n",
    "- ROUGE也分为四种方法:\n",
    "    - ROUGE-N (N=1,2,3,4)\n",
    "    - ROUGE-L\n",
    "    - ROUGE-W\n",
    "    - ROUGE-S\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./assets/Evaluation-Output.jpg\" style=\"margin-left: 0px\" width=1024px>"
   ],
   "id": "adb81139e8f1a23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. LLaMA-Factory QLoRA: Quantization at training stage\n",
    "\n",
    "## 2.1 What is the problem to solve by this advanced LoRA?\n",
    "\n",
    "### If we are using a 4090 GPU with 24GB memory to train a 1B model with larger batch size:\n",
    "\n",
    "<img src=\"./assets/Out-Of-Memory.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### We will run out of memory, like this:\n",
    "\n",
    "```bash\n",
    "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.38 GiB. GPU 0 has a total capacity of 23.55 GiB of which 5.32 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 16.66 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "  0%|          | 0/600 [00:01<?, ?it/s]\n",
    "```\n",
    "\n",
    "## 2.2 In this case, we can use `quantization` to reduce the model size and memory usage.\n",
    "\n",
    "- QLoRA: none -> 8,\n",
    "- LoRA Configuration: Rank=8 -> 32, alpha(scaling coefficient)=16 -> 64,\n",
    "\n",
    "<img src=\"./assets/QLoRA-Configuration.jpg\" style=\"margin-left: 0px\" width=1024px>\n"
   ],
   "id": "55ef1230c5ddb52f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.3 There was a dependency issue with `bitsandbytes`, Need to be set `0.44.1` version.\n",
    "```bash\n",
    "Traceback (most recent call last):\n",
    "  File \"/root/miniconda3/bin/llamafactory-cli\", line 8, in <module>\n",
    "    sys.exit(main())\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/cli.py\", line 112, in main\n",
    "    run_exp()\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 92, in run_exp\n",
    "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 66, in _training_function\n",
    "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 52, in run_sft\n",
    "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/model/loader.py\", line 170, in load_model\n",
    "    model = init_adapter(config, model, model_args, finetuning_args, is_trainable)\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/model/adapter.py\", line 299, in init_adapter\n",
    "    model = _setup_lora_tuning(\n",
    "  File \"/root/autodl-tmp/LLaMA-Factory/src/llamafactory/model/adapter.py\", line 250, in _setup_lora_tuning\n",
    "    model = get_peft_model(model, lora_config)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/mapping.py\", line 183, in get_peft_model\n",
    "    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/peft_model.py\", line 1542, in __init__\n",
    "    super().__init__(model, peft_config, adapter_name, **kwargs)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/peft_model.py\", line 155, in __init__\n",
    "    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/model.py\", line 139, in __init__\n",
    "    super().__init__(model, config, adapter_name)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 175, in __init__\n",
    "    self.inject_adapter(self.model, adapter_name)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 431, in inject_adapter\n",
    "    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/model.py\", line 224, in _create_and_replace\n",
    "    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/model.py\", line 340, in _create_new_module\n",
    "    new_module = dispatcher(target, adapter_name, lora_config=lora_config, **kwargs)\n",
    "  File \"/root/miniconda3/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 273, in dispatch_bnb_8bit\n",
    "    \"memory_efficient_backward\": target.state.memory_efficient_backward,\n",
    "AttributeError: 'MatmulLtState' object has no attribute 'memory_efficient_backward'\n",
    "```"
   ],
   "id": "279071c9c778bd7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install bitsandbytes==0.44.1",
   "id": "d5edc3445b2ce667"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.4 Now, we can see trigger the QLoRA training process.\n",
    "\n",
    "<img src=\"./assets/QLoRA-Training.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "## 2.5 `nvitop` command to monitor GPU usage:\n",
    "\n",
    "<img src=\"./assets/nvitop.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "## 2.6 LLAMA-Factory logging:"
   ],
   "id": "8b62d0557b7b6311"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "root@autodl-container-fb0c4680e1-702ff0ba:~/autodl-tmp/LLaMA-Factory# llamafactory-cli webui\n",
    "Running on local URL:  http://0.0.0.0:7861\n",
    "\n",
    "To create a public link, set `share=True` in `launch()`.\n",
    "[WARNING|2025-01-19 06:25:38] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.\n",
    "[INFO|2025-01-19 06:25:38] llamafactory.hparams.parser:373 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
    "[INFO|configuration_utils.py:677] 2025-01-19 06:25:38,566 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/config.json\n",
    "[INFO|configuration_utils.py:746] 2025-01-19 06:25:38,567 >> Model config LlamaConfig {\n",
    "  \"_name_or_path\": \"/root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B\",\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"head_dim\": 64,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 2048,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 8192,\n",
    "  \"max_position_embeddings\": 131072,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 32,\n",
    "  \"num_hidden_layers\": 16,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_scaling\": {\n",
    "    \"factor\": 32.0,\n",
    "    \"high_freq_factor\": 4.0,\n",
    "    \"low_freq_factor\": 1.0,\n",
    "    \"original_max_position_embeddings\": 8192,\n",
    "    \"rope_type\": \"llama3\"\n",
    "  },\n",
    "  \"rope_theta\": 500000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.46.1\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 128256\n",
    "}\n",
    "\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,568 >> loading file tokenizer.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,568 >> loading file tokenizer.model\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,568 >> loading file added_tokens.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,569 >> loading file special_tokens_map.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,569 >> loading file tokenizer_config.json\n",
    "[INFO|tokenization_utils_base.py:2475] 2025-01-19 06:25:38,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "[INFO|configuration_utils.py:677] 2025-01-19 06:25:38,935 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/config.json\n",
    "[INFO|configuration_utils.py:746] 2025-01-19 06:25:38,936 >> Model config LlamaConfig {\n",
    "  \"_name_or_path\": \"/root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B\",\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"head_dim\": 64,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 2048,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 8192,\n",
    "  \"max_position_embeddings\": 131072,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 32,\n",
    "  \"num_hidden_layers\": 16,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_scaling\": {\n",
    "    \"factor\": 32.0,\n",
    "    \"high_freq_factor\": 4.0,\n",
    "    \"low_freq_factor\": 1.0,\n",
    "    \"original_max_position_embeddings\": 8192,\n",
    "    \"rope_type\": \"llama3\"\n",
    "  },\n",
    "  \"rope_theta\": 500000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.46.1\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 128256\n",
    "}\n",
    "\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,937 >> loading file tokenizer.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,937 >> loading file tokenizer.model\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,937 >> loading file added_tokens.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,937 >> loading file special_tokens_map.json\n",
    "[INFO|tokenization_utils_base.py:2209] 2025-01-19 06:25:38,937 >> loading file tokenizer_config.json\n",
    "[INFO|tokenization_utils_base.py:2475] 2025-01-19 06:25:39,281 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "[INFO|2025-01-19 06:25:39] llamafactory.data.template:157 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
    "[INFO|2025-01-19 06:25:39] llamafactory.data.loader:157 >> Loading dataset ruozhiba.json...\n",
    "training example:\n",
    "input_ids:\n",
    "[128000, 128006, 882, 128007, 271, 92780, 122679, 48044, 64209, 101171, 237, 35287, 98806, 27327, 76706, 103054, 11571, 128009, 128006, 78191, 128007, 271, 27327, 3922, 17792, 22656, 37507, 81258, 112986, 48044, 64209, 101171, 237, 1811, 128009]\n",
    "inputs:\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "只剩一个心脏了还能活吗？<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "能，人本来就只有一个心脏。<|eot_id|>\n",
    "label_ids:\n",
    "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 27327, 3922, 17792, 22656, 37507, 81258, 112986, 48044, 64209, 101171, 237, 1811, 128009]\n",
    "labels:\n",
    "能，人本来就只有一个心脏。<|eot_id|>\n",
    "[INFO|configuration_utils.py:677] 2025-01-19 06:25:40,237 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/config.json\n",
    "[INFO|configuration_utils.py:746] 2025-01-19 06:25:40,238 >> Model config LlamaConfig {\n",
    "  \"_name_or_path\": \"/root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B\",\n",
    "  \"architectures\": [\n",
    "    \"LlamaForCausalLM\"\n",
    "  ],\n",
    "  \"attention_bias\": false,\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"head_dim\": 64,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 2048,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 8192,\n",
    "  \"max_position_embeddings\": 131072,\n",
    "  \"mlp_bias\": false,\n",
    "  \"model_type\": \"llama\",\n",
    "  \"num_attention_heads\": 32,\n",
    "  \"num_hidden_layers\": 16,\n",
    "  \"num_key_value_heads\": 8,\n",
    "  \"pretraining_tp\": 1,\n",
    "  \"rms_norm_eps\": 1e-05,\n",
    "  \"rope_scaling\": {\n",
    "    \"factor\": 32.0,\n",
    "    \"high_freq_factor\": 4.0,\n",
    "    \"low_freq_factor\": 1.0,\n",
    "    \"original_max_position_embeddings\": 8192,\n",
    "    \"rope_type\": \"llama3\"\n",
    "  },\n",
    "  \"rope_theta\": 500000.0,\n",
    "  \"tie_word_embeddings\": true,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.46.1\",\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 128256\n",
    "}\n",
    "\n",
    "[INFO|2025-01-19 06:25:40] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 8 bit with bitsandbytes.\n",
    "[INFO|modeling_utils.py:3934] 2025-01-19 06:25:40,338 >> loading weights file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/model.safetensors.index.json\n",
    "[INFO|modeling_utils.py:1670] 2025-01-19 06:25:40,338 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
    "[INFO|configuration_utils.py:1096] 2025-01-19 06:25:40,339 >> Generate config GenerationConfig {\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ]\n",
    "}\n",
    "\n",
    "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.37it/s]\n",
    "[INFO|modeling_utils.py:4800] 2025-01-19 06:25:41,879 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
    "\n",
    "[INFO|modeling_utils.py:4808] 2025-01-19 06:25:41,879 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B.\n",
    "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
    "[INFO|configuration_utils.py:1049] 2025-01-19 06:25:41,881 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/generation_config.json\n",
    "[INFO|configuration_utils.py:1096] 2025-01-19 06:25:41,881 >> Generate config GenerationConfig {\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"do_sample\": true,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"temperature\": 0.6,\n",
    "  \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "[INFO|2025-01-19 06:25:41] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
    "[INFO|2025-01-19 06:25:41] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
    "[INFO|2025-01-19 06:25:41] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
    "[INFO|2025-01-19 06:25:41] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
    "[INFO|2025-01-19 06:25:41] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,q_proj,down_proj,o_proj,v_proj,up_proj,k_proj\n",
    "[INFO|2025-01-19 06:25:42] llamafactory.model.loader:157 >> trainable params: 22,544,384 || all params: 1,258,358,784 || trainable%: 1.7916\n",
    "[INFO|trainer.py:698] 2025-01-19 06:25:42,206 >> Using auto half precision backend\n",
    "[INFO|trainer.py:2313] 2025-01-19 06:25:42,435 >> ***** Running training *****\n",
    "[INFO|trainer.py:2314] 2025-01-19 06:25:42,435 >>   Num examples = 1,496\n",
    "[INFO|trainer.py:2315] 2025-01-19 06:25:42,435 >>   Num Epochs = 100\n",
    "[INFO|trainer.py:2316] 2025-01-19 06:25:42,435 >>   Instantaneous batch size per device = 30\n",
    "[INFO|trainer.py:2319] 2025-01-19 06:25:42,435 >>   Total train batch size (w. parallel, distributed & accumulation) = 240\n",
    "[INFO|trainer.py:2320] 2025-01-19 06:25:42,435 >>   Gradient Accumulation steps = 8\n",
    "[INFO|trainer.py:2321] 2025-01-19 06:25:42,435 >>   Total optimization steps = 600\n",
    "[INFO|trainer.py:2322] 2025-01-19 06:25:42,437 >>   Number of trainable parameters = 22,544,384\n",
    "  0%|                                                                                                                    | 0/600 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
    "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
    "  1%|▉                                                                                                         | 5/600 [00:32<1:02:46,  6.33s/it][INFO|2025-01-19 06:26:15] llamafactory.train.callbacks:157 >> {'loss': 1.7882, 'learning_rate': 4.9991e-05, 'epoch': 0.80}\n",
    "{'loss': 1.7882, 'grad_norm': 0.5794951915740967, 'learning_rate': 4.999143312438893e-05, 'epoch': 0.8}\n",
    "  2%|█▊                                                                                                       | 10/600 [01:04<1:02:30,  6.36s/it][INFO|2025-01-19 06:26:46] llamafactory.train.callbacks:157 >> {'loss': 1.9503, 'learning_rate': 4.9966e-05, 'epoch': 1.60}\n",
    "{'loss': 1.9503, 'grad_norm': 0.4185701608657837, 'learning_rate': 4.996573836886435e-05, 'epoch': 1.6}\n",
    "  2%|██▋                                                                                                      | 15/600 [01:38<1:06:23,  6.81s/it][INFO|2025-01-19 06:27:20] llamafactory.train.callbacks:157 >> {'loss': 1.8624, 'learning_rate': 4.9923e-05, 'epoch': 2.40}\n",
    "{'loss': 1.8624, 'grad_norm': 0.4220849573612213, 'learning_rate': 4.99229333433282e-05, 'epoch': 2.4}\n",
    "  3%|███▌                                                                                                       | 20/600 [02:08<59:52,  6.19s/it][INFO|2025-01-19 06:27:51] llamafactory.train.callbacks:157 >> {'loss': 1.8104, 'learning_rate': 4.9863e-05, 'epoch': 3.20}\n",
    "{'loss': 1.8104, 'grad_norm': 0.35836485028266907, 'learning_rate': 4.9863047384206835e-05, 'epoch': 3.2}\n",
    "  4%|████▍                                                                                                    | 25/600 [02:41<1:03:02,  6.58s/it][INFO|2025-01-19 06:28:24] llamafactory.train.callbacks:157 >> {'loss': 1.7765, 'learning_rate': 4.9786e-05, 'epoch': 4.00}\n",
    "{'loss': 1.7765, 'grad_norm': 0.8218790292739868, 'learning_rate': 4.9786121534345265e-05, 'epoch': 4.0}\n",
    "  5%|█████▎                                                                                                   | 30/600 [03:15<1:02:37,  6.59s/it][INFO|2025-01-19 06:28:57] llamafactory.train.callbacks:157 >> {'loss': 1.5137, 'learning_rate': 4.9692e-05, 'epoch': 4.80}\n",
    "{'loss': 1.5137, 'grad_norm': 0.3808193504810333, 'learning_rate': 4.9692208514878444e-05, 'epoch': 4.8}\n",
    "  6%|██████▏                                                                                                  | 35/600 [03:47<1:00:57,  6.47s/it][INFO|2025-01-19 06:29:29] llamafactory.train.callbacks:157 >> {'loss': 1.6818, 'learning_rate': 4.9581e-05, 'epoch': 5.60}\n",
    "{'loss': 1.6818, 'grad_norm': 0.37492719292640686, 'learning_rate': 4.958137268909887e-05, 'epoch': 5.6}\n",
    "  7%|███████                                                                                                  | 40/600 [04:18<1:00:34,  6.49s/it][INFO|2025-01-19 06:30:01] llamafactory.train.callbacks:157 >> {'loss': 1.6692, 'learning_rate': 4.9454e-05, 'epoch': 6.40}\n",
    "{'loss': 1.6692, 'grad_norm': 0.3768139183521271, 'learning_rate': 4.9453690018345144e-05, 'epoch': 6.4}\n",
    "  7%|███████▏                                                                                                 | 41/600 [04:25<1:00:06,  6.45s/it]\n",
    "```"
   ],
   "id": "e8f9b52e37709217"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.7 After 600 steps training, we can see the training process is finished.\n",
    "\n",
    "<img src=\"./assets/QLoRA-Training-Completed.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "## 2.8 Export Model with merged training parameters: (Only QLoRA trained + all parameters merged)\n",
    "\n",
    "<img src=\"./assets/QLoRA-Training-Output1.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### Logging:\n",
    "\n",
    "```bash\n",
    "[INFO|modeling_utils.py:4800] 2025-01-19 07:33:56,855 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
    "\n",
    "[INFO|modeling_utils.py:4808] 2025-01-19 07:33:56,855 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B.\n",
    "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
    "[INFO|configuration_utils.py:1049] 2025-01-19 07:33:56,857 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B/generation_config.json\n",
    "[INFO|configuration_utils.py:1096] 2025-01-19 07:33:56,857 >> Generate config GenerationConfig {\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"do_sample\": true,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"temperature\": 0.6,\n",
    "  \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "[INFO|2025-01-19 07:33:56] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
    "[INFO|2025-01-19 07:33:57] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
    "[INFO|2025-01-19 07:33:57] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.2-1B/lora/train_2025-01-19-06-24-25\n",
    "[INFO|2025-01-19 07:33:57] llamafactory.model.loader:157 >> all params: 1,235,814,400\n",
    "[INFO|2025-01-19 07:33:57] llamafactory.train.tuner:157 >> Convert model dtype to: torch.float32.\n",
    "[INFO|configuration_utils.py:414] 2025-01-19 07:33:57,380 >> Configuration saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/config.json\n",
    "[INFO|configuration_utils.py:865] 2025-01-19 07:33:57,381 >> Configuration saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/generation_config.json\n",
    "[INFO|modeling_utils.py:3043] 2025-01-19 07:34:05,040 >> The model is bigger than the maximum size per checkpoint (3GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/model.safetensors.index.json.\n",
    "[INFO|tokenization_utils_base.py:2646] 2025-01-19 07:34:05,043 >> tokenizer config file saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/tokenizer_config.json\n",
    "[INFO|tokenization_utils_base.py:2655] 2025-01-19 07:34:05,043 >> Special tokens file saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/special_tokens_map.json\n",
    "```\n",
    "\n",
    "## 2.9 Export Model with quantization parameters: (QLoRA trained + quantized export)\n",
    "\n",
    "<img src=\"./assets/QLoRA-Training-Output2.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### Logging:\n",
    "\n",
    "```bash\n",
    "[INFO|modeling_utils.py:3934] 2025-01-19 07:36:34,187 >> loading weights file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/model.safetensors.index.json\n",
    "[INFO|modeling_utils.py:1670] 2025-01-19 07:36:34,187 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
    "[INFO|configuration_utils.py:1096] 2025-01-19 07:36:34,399 >> Generate config GenerationConfig {\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ]\n",
    "}\n",
    "\n",
    "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.82it/s]\n",
    "[INFO|modeling_utils.py:4800] 2025-01-19 07:36:35,532 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
    "\n",
    "[INFO|modeling_utils.py:4808] 2025-01-19 07:36:35,532 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA.\n",
    "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
    "[INFO|configuration_utils.py:1049] 2025-01-19 07:36:35,534 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA/generation_config.json\n",
    "[INFO|configuration_utils.py:1096] 2025-01-19 07:36:35,534 >> Generate config GenerationConfig {\n",
    "  \"bos_token_id\": 128000,\n",
    "  \"do_sample\": true,\n",
    "  \"eos_token_id\": [\n",
    "    128001,\n",
    "    128008,\n",
    "    128009\n",
    "  ],\n",
    "  \"temperature\": 0.6,\n",
    "  \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "Quantizing model.layers blocks : 100%|███████████████████████████████████████████████████████████████████████████| 16/16 [01:39<00:00,  6.22s/it]\n",
    "/root/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
    "  warnings.warn(\n",
    "[WARNING|logging.py:328] 2025-01-19 07:38:15,264 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
    "[INFO|2025-01-19 07:38:54] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
    "[INFO|2025-01-19 07:38:54] llamafactory.model.loader:157 >> all params: 262,735,872\n",
    "[INFO|configuration_utils.py:414] 2025-01-19 07:38:54,432 >> Configuration saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/config.json\n",
    "[INFO|configuration_utils.py:865] 2025-01-19 07:38:54,433 >> Configuration saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/generation_config.json\n",
    "[INFO|modeling_utils.py:3035] 2025-01-19 07:38:56,364 >> Model weights saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/model.safetensors\n",
    "[INFO|tokenization_utils_base.py:2646] 2025-01-19 07:38:56,368 >> tokenizer config file saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/tokenizer_config.json\n",
    "[INFO|tokenization_utils_base.py:2655] 2025-01-19 07:38:56,368 >> Special tokens file saved in /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/special_tokens_map.json\n",
    "```"
   ],
   "id": "839e97470f8e0201"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Evaluate the fine-tuned model\n",
    "\n",
    "### 3.1 Evaluate QLoRA with Chat:\n",
    "\n",
    "<img src=\"./assets/QLoRA-Evaluation-Chat.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### 3.2 Evaluate QLoRA with Valuation Dataset:\n",
    "\n",
    "<img src=\"./assets/QLoRA-Evaluation-Predict.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### 3.3 Evaluate QLoRA Quantized export model:\n",
    "\n",
    "<img src=\"./assets/QLoRA-Evaluation-Quantized.jpg\" style=\"margin-left: 0px\" width=1024px>\n",
    "\n",
    "### Logging:\n",
    "\n",
    "```bash\n",
    "[INFO|2025-01-19 18:52:49] logging.py:157 >> Loading 8-bit GPTQ-quantized model.\n",
    "\n",
    "[INFO|2025-01-19 18:52:49] logging.py:157 >> Using KV cache for faster generation.\n",
    "\n",
    "[INFO|2025-01-19 18:52:49] modeling_utils.py:3934 >> loading weights file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/model.safetensors\n",
    "\n",
    "[INFO|2025-01-19 18:52:49] modeling_utils.py:1670 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
    "\n",
    "[INFO|2025-01-19 18:52:49] configuration_utils.py:1096 >> Generate config GenerationConfig { \"bos_token_id\": 128000, \"eos_token_id\": [ 128001, 128008, 128009 ] }\n",
    "\n",
    "[WARNING|2025-01-19 18:52:49] logging.py:328 >> loss_type=None was set in the config but it is unrecognised.Using the default loss: ForCausalLMLoss.\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] modeling_utils.py:4800 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] modeling_utils.py:4808 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8. If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] configuration_utils.py:1049 >> loading configuration file /root/autodl-tmp/models/UnicomAI/Unichat-llama3.2-Chinese-1B-QLoRA-Q8/generation_config.json\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] configuration_utils.py:1096 >> Generate config GenerationConfig { \"bos_token_id\": 128000, \"do_sample\": true, \"eos_token_id\": [ 128001, 128008, 128009 ], \"temperature\": 0.6, \"top_p\": 0.9 }\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] logging.py:157 >> Using torch SDPA for faster training and inference.\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] logging.py:157 >> all params: 262,735,872\n",
    "\n",
    "[WARNING|2025-01-19 18:52:50] logging.py:168 >> Batch generation can be very slow. Consider using scripts/vllm_infer.py instead.\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] trainer.py:4117 >> ***** Running Prediction *****\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] trainer.py:4119 >> Num examples = 8\n",
    "\n",
    "[INFO|2025-01-19 18:52:50] trainer.py:4122 >> Batch size = 2\n",
    "```"
   ],
   "id": "5032dff016d2bbec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bbe19cf8d1dc0c9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
