{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hugging Face BERT Fine-Tuning with PyTorch\n",
    "\n",
    "## 1. Download Pre-trained Model\n",
    "- Download the pre-trained BERT model from the Hugging Face model hub.\n",
    "- We will do fine-tuning on top of it for the sentiment analysis task."
   ],
   "id": "e80c5bdd659f396a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"google-bert/bert-base-chinese\"\n",
    "cache_dir = \"../local_models\"\n",
    "AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)"
   ],
   "id": "c337dd2b14040467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Define Dataset class, for loading our custom dataset for further fine-tuning\n",
    "- The dataset should be prepared in advance, and in different purpose, like train, validation, test, etc.\n",
    "- There should more data for training, smaller size for validation and test."
   ],
   "id": "2a9ec39572186a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_type, dataset_path):\n",
    "        self.dataset = load_from_disk(dataset_path)\n",
    "        if dataset_type == 'train':\n",
    "            self.dataset = self.dataset[\"train\"]\n",
    "        elif dataset_type == 'validation':\n",
    "            self.dataset = self.dataset[\"validation\"]\n",
    "        elif dataset_type == 'test':\n",
    "            self.dataset = self.dataset[\"test\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.dataset[item]['text']\n",
    "        label = self.dataset[item]['label']\n",
    "        return text,label\n",
    "\n",
    "dataset_train = MyDataset(\"train\", r\"../local_datasets/ChnSentiCorp\")\n",
    "for data in dataset_train[:5]:\n",
    "    print(data)\n",
    "\n",
    "dataset_validation = MyDataset(\"validation\", r\"../local_datasets/ChnSentiCorp\")\n",
    "for data in dataset_validation[:5]:\n",
    "    print(data)\n",
    "\n",
    "dataset_test = MyDataset(\"test\", r\"../local_datasets/ChnSentiCorp\")\n",
    "for data in dataset_test[:5]:\n",
    "    print(data)"
   ],
   "id": "6644c379356e8bc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Define downstream tasks model\n",
    "- Extending the pretrained model for the fine-tuning"
   ],
   "id": "67e7ed98e125e1b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "pretrained_model = (\n",
    "    BertModel\n",
    "    .from_pretrained(r\"../local_models/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\")\n",
    "    .to(DEVICE)\n",
    ")\n",
    "print(pretrained_model)\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        # Freeze Pretrained model's parameters, don't engage in the fine-tuning train.\n",
    "        with torch.no_grad():\n",
    "            out = pretrained_model(input_ids=input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        # Incremental model\n",
    "        out = self.fc(out.last_hidden_state[:,0])\n",
    "        print(out)\n",
    "        return out"
   ],
   "id": "ae1a7c1fba42c5f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 4. Training\n"
   ],
   "id": "7d1e4532f22a9539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer,AdamW\n",
    "import torch\n",
    "\n",
    "# Previous step loaded the DEVICE already, otherwise, you can load it again.\n",
    "# DEVICE = torch.device(\"cuba\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer to encode the data\n",
    "tokenizer = (\n",
    "    BertTokenizer\n",
    "    .from_pretrained(r\"../local_models/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\")\n",
    ")\n",
    "\n",
    "# to encode the data while loading process\n",
    "def tokenize_batches(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=texts,\n",
    "        truncation=True,\n",
    "        max_length=500,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        return_length=True\n",
    "    )\n",
    "    tensor_labels = torch.tensor(labels)\n",
    "    return encoded_data[\"input_ids\"], encoded_data[\"attention_mask\"], encoded_data[\"token_type_ids\"], tensor_labels\n",
    "\n",
    "# loading the training dataset\n",
    "train_data_loader = DataLoader (\n",
    "    dataset_train,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "# loading the validation dataset\n",
    "validation_data_loader = DataLoader (\n",
    "    dataset_validation,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "EPOCH = 2 # This should be very large, like 30000, but for the demo, we set it to 3.\n",
    "def run_training(data_loader=train_data_loader):\n",
    "    print(DEVICE)\n",
    "    model = MyModel().to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    best_validation_acc = 0.0\n",
    "    for epoch in range(EPOCH):\n",
    "        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(data_loader):\n",
    "            input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            loss = loss_func(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out==labels).sum().item()/len(labels)\n",
    "                print(f\"epoch:{epoch},i:{i},loss:{loss.item()},acc:{acc}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validation_acc = 0.0\n",
    "            validation_loss = 0.0\n",
    "            for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(validation_data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "                out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                validation_loss += loss_func(out, labels)\n",
    "                out = out.argmax(dim=1)\n",
    "                validation_acc += (out==labels).sum().item()\n",
    "            validation_loss /= len(data_loader)\n",
    "            validation_acc /= len(data_loader)\n",
    "            print(f\"epoch:{epoch},validation_loss:{validation_loss},validation_acc:{validation_acc}\")\n",
    "\n",
    "            if validation_acc > best_validation_acc:\n",
    "                best_validation_acc = validation_acc\n",
    "                torch.save(model.state_dict(), \"params/best.pth\")\n",
    "                print(f\"epoch:{epoch},best model saved with acc:{best_validation_acc}\")\n",
    "\n",
    "        torch.save(model.state_dict(), \"params/last.pth\")\n",
    "        print(f\"epoch:{epoch},last model saved\")\n",
    "\n",
    "run_training()"
   ],
   "id": "7c8a0e328f9db0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Testing\n",
    "- After training, we need to test the model on the test dataset.\n",
    "- Load the generated parameters model and test it"
   ],
   "id": "757089eb3ab4f86f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "test_data_loader = DataLoader (\n",
    "    dataset_test,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "def run_testing(param_path=\"params/best.pth\"):\n",
    "    test_acc = 0.0\n",
    "    total = 0\n",
    "    model = MyModel().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(param_path))\n",
    "    model.eval()\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(test_data_loader):\n",
    "        input_ids, attention_mask, token_type_ids = input_ids.to(DEVICE), attention_mask.to(DEVICE), token_type_ids.to(DEVICE)\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        out = out.argmax(dim=1)\n",
    "        test_acc += (out==labels).sum().item()\n",
    "        print(i, (out==labels).sum().item())\n",
    "        total += len(labels)\n",
    "    print(f\"test_acc:{test_acc/total}\")\n",
    "\n",
    "run_testing()\n",
    "run_testing(\"params/last.pth\")"
   ],
   "id": "b621f7e78e93bdac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extra 1. Customize the encoding vocabulary\n",
    "- To extend the default vocabulary of the pre-trained model, you can add new tokens to the vocabulary.\n",
    "- After adding new tokens, you will need training the model again."
   ],
   "id": "e3574c4d4bfde105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# How tokenizer works\n",
    "tokenizer = (\n",
    "    BertTokenizer\n",
    "    .from_pretrained(r\"../local_models/models--google-bert--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\")\n",
    ")\n",
    "previous_out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[\"阳光洒在大地上\"],\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=20,\n",
    "    return_length=None\n",
    ")\n",
    "print(previous_out[\"input_ids\"][0])\n",
    "print(tokenizer.decode(previous_out[\"input_ids\"][0]))\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.vocab\n",
    "print(len(vocab))\n",
    "print('阳' in vocab)\n",
    "print('光' in vocab)\n",
    "print('阳光' in vocab)\n",
    "\n",
    "# Add new tokens to the vocabulary\n",
    "tokenizer.add_tokens(new_tokens=[\"阳光\"])\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))\n",
    "print('阳' in vocab)\n",
    "print('光' in vocab)\n",
    "print('阳光' in vocab)\n",
    "\n",
    "# Encode the sentence again\n",
    "out = tokenizer.batch_encode_plus(\n",
    "    batch_text_or_text_pairs=[\"阳光洒在大地上\"],\n",
    "    add_special_tokens=True,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=20,\n",
    "    return_length=None\n",
    ")\n",
    "\n",
    "print(previous_out[\"input_ids\"][0])\n",
    "print(out[\"input_ids\"][0])\n",
    "print(tokenizer.decode(out[\"input_ids\"][0]))"
   ],
   "id": "2dafea7da14eb4bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extra 2. Bad Dataset Test\n",
    "- If the dataset is not well-prepared, the model will not work as expected."
   ],
   "id": "21bbe77a958f6b2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class CsvDataset(Dataset):\n",
    "    def __init__(self,file_path):\n",
    "        #从磁盘加载csv数据\n",
    "        self.dataset = load_dataset(path=\"csv\",data_files=file_path,split=\"train\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.dataset[item][\"text\"]\n",
    "        label = self.dataset[item][\"label\"]\n",
    "\n",
    "        return text,label\n",
    "\n",
    "#TODO This dataset's labels are more than 2 classes, but the model is designed for binary classification.\n",
    "dataset_bad = CsvDataset(f\"../local_datasets/Weibo/test.csv\")\n",
    "for data in dataset_bad[:5]:\n",
    "    print(data)\n",
    "\n",
    "bad_data_loader = DataLoader (\n",
    "    dataset_bad,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=tokenize_batches\n",
    ")\n",
    "\n",
    "run_training(bad_data_loader)"
   ],
   "id": "33a27085ea4d72fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
