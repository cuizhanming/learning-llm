{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Choose a GPU Server: `Nvidia4090 24G`\n",
    "\n",
    "Ideally, we should have different servers for different purposes, since Python environment is so fragile.\n",
    "Otherwise, using `Conda` to manage isolated pythong environment per frameworks:\n",
    "- Training frameworks\n",
    "- Inference frameworks\n",
    "- Deployment frameworks"
   ],
   "id": "407f7d8a7c3982eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Prepare the pre-trained model and dataset\n",
    "\n",
    "### 2.1 HuggingFace `https://huggingface.co/`\n",
    "\n",
    "- Option 1: Python `transformers` library\n",
    "```bash\n",
    "# Download model\n",
    "# !pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-1.8B-Chat\")\n",
    "\n",
    "# Download dataset\n",
    "wget https://hf-mirror.com/datasets/LooksJuicy/ruozhiba/blob/main/ruozhiba_qa.json\n",
    "```\n",
    "\n",
    "### 2.2 ModelScope `https://modelscope.cn/`\n",
    "\n",
    "- Option 1: Python `modelscope` library\n",
    "```bash\n",
    "# Download model\n",
    "# !pip install modelscope\n",
    "from modelscope import snapshot_download\n",
    "snapshot_download('Qwen/Qwen1.5-1.8B-Chat', cache_dir='/root/autodl-tmp/models')\n",
    "\n",
    "# Download dataset\n",
    "wget https://hf-mirror.com/datasets/LooksJuicy/ruozhiba/blob/main/ruozhiba_qa.json\n",
    "```\n",
    "\n",
    "### 2.3 HuggingFace Mirror `https://hf-mirror.com`\n",
    "\n",
    "- Option 1: hfd.sh tool\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "# download CLI tool\n",
    "wget https://hf-mirror.com/hfd/hfd.sh\n",
    "chmod a+x hfd.sh\n",
    "\n",
    "# download models\n",
    "mkdir -p /root/autodl-tmp/models\n",
    "cd /root/autodl-tmp/models\n",
    "./hfd.sh Qwen/Qwen1.5-1.8B-Chat\n",
    "\n",
    "# download datasets\n",
    "mkdir -p /root/autodl-tmp/datasets\n",
    "cd /root/autodl-tmp/datasets\n",
    "./hfd.sh LooksJuicy/ruozhiba --dataset\n",
    "```"
   ],
   "id": "8b0bb207f47ffbaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Formatting dataset as LLaMA-Factory supports",
   "id": "3a19286811c7a3f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "def convert_json_format(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "        item['instruction'] = item.pop('query')\n",
    "        item['input'] = ''\n",
    "        item['output'] = item.pop('response')\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "input_file = '/root/autodl-tmp/datasets/ruozhiba_qaswift.json'\n",
    "output_file = '/root/autodl-tmp/datasets/ruozhiba.json'\n",
    "\n",
    "convert_json_format(input_file, output_file)"
   ],
   "id": "3e4d32727ba5a510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Choose fine-tuning tool: `LLaMA-Factory`\n",
    "\n",
    "- Install LLaMA-Factory\n",
    "```bash\n",
    "# In case `source /etc/network_turbo` for autodl.com is enabled, reset it\n",
    "unset http_proxy && unset https_proxy\n",
    "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "cd LLaMA-Factory\n",
    "\n",
    "# Store the packages in give PATH to manage the storage on Server\n",
    "conda create -n llama-factory -p /root/autodl-tmp/penv/llama-factory python==3.12\n",
    "conda activate llama-factory\n",
    "pip install -e \".[torch,metrics]\"\n",
    "```\n",
    "- Run LLaMA-Factory webUI\n",
    "```bash\n",
    "nohup llamafactory-cli webui &\n",
    "```"
   ],
   "id": "f1daebb90f16434f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Configure and start fine-tuning process: `QLoRA`\n",
    "- Specific package version\n",
    "```bash\n",
    "pip install bitsandbytes==0.44.1\n",
    "```"
   ],
   "id": "3cfb6654f5918d31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluate the fine-tuned model\n",
    "\n",
    "- Chat with Hugging Face inference framework\n",
    "- Chat with VLLM inference framework\n",
    "```bash\n",
    "pip install -e \".[vllm]\"\n",
    "```"
   ],
   "id": "723faedbac5698c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Export the fine-tuned model",
   "id": "5aadf5edbc7930f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Convert model format from HF to GGUF: [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "- Download llama.cpp\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "```\n",
    "\n",
    "- Install requirements\n",
    "```bash\n",
    "source /etc/network_turbo\n",
    "\n",
    "conda create -n llama-cpp -p /root/autodl-tmp/penv/llama-cpp python==3.12\n",
    "conda activate llama-cpp\n",
    "\n",
    "pip install -r llama.cpp/requirements.txt\n",
    "# In case missing these from requirements.txt\n",
    "pip install sentencepiece\n",
    "pip install safetensors\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "- Convert model format\n",
    "```bash\n",
    "MODEL=./path/to/the/model/model_name\n",
    "GGUF=./path/to/the/gguf/file/model_name.gguf\n",
    "python llama.cpp/convert_hf_to_gguf.py $MODEL --outtype f16 --verbose --outfile $GGUF\n",
    "```"
   ],
   "id": "a30692453dd5ab8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Deploy the fine-tuned model: [`Ollama`](https://ollama.com/)\n",
    "\n",
    "- Install Ollama\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "- Create Ollama `ModelFile` with below content\n",
    "```text\n",
    "FROM /path/to/the/gguf/file/model-name.gguf\n",
    "```\n",
    "\n",
    "- Create Ollama Model\n",
    "```bash\n",
    "ollama create model-name --file ./ModelFile\n",
    "```\n",
    "\n",
    "- Start Ollama Model\n",
    "```bash\n",
    "ollama run model-name\n",
    "```"
   ],
   "id": "8b99c6936b350c29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Deploy [`OpenWebUI`](https://github.com/open-webui/open-webui)\n",
    "\n",
    "- Create Conda environment\n",
    "```bash\n",
    "# Store the packages in give PATH to manage the storage on Server\n",
    "conda create -n open-webui -p /root/autodl-tmp/penv/open-webui python==3.11\n",
    "conda init\n",
    "conda activate open-webui\n",
    "\n",
    "pip install -U open-webui\n",
    "```\n",
    "\n",
    "- Start `OpenWebUI` as a service\n",
    "```bash\n",
    "# Used for initiating the framework\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "# Used for polling all models from OpenAI or Ollama\n",
    "export OPENAI_API_BASE_URL=http://127.0.0.1:11434/v1\n",
    "# This is default value already, just to be aware.\n",
    "export ENABLE_OLLAMA_API=True\n",
    "# Can be ignored, and configure in the WebUI later\n",
    "export DEFAULT_MODELS=\"/path/to/the/model\"\n",
    "\n",
    "open-webui serve\n",
    "```\n",
    "\n",
    "- Docker container with both `Ollama` and `OpenWebUI`\n",
    "```bash\n",
    "docker run -d -p 3000:8080 \\n\n",
    "-v ollama:/root/.ollama \\n\n",
    "-v open-webui:/app/backend/data \\n\n",
    "--name open-webui \\n\n",
    "--restart always \\n\n",
    "ghcr.io/open-webui/open-webui:ollama\n",
    "```"
   ],
   "id": "dba7bd669d36cb88"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
